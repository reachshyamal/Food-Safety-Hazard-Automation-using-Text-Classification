{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/Pepsico/input_data.csv',index_col=False, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>projName</th>\n",
       "      <th>accolNumber</th>\n",
       "      <th>PDA_projName</th>\n",
       "      <th>projType</th>\n",
       "      <th>projDesc</th>\n",
       "      <th>formulaNumber</th>\n",
       "      <th>owner</th>\n",
       "      <th>sector</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>baseProduct</th>\n",
       "      <th>prodModifications</th>\n",
       "      <th>newIngradient</th>\n",
       "      <th>approvedPackage</th>\n",
       "      <th>potentialMicrobial</th>\n",
       "      <th>cross_contactAllergens</th>\n",
       "      <th>chokeHazard</th>\n",
       "      <th>operationalAllergen</th>\n",
       "      <th>Unnamed: 44</th>\n",
       "      <th>Unnamed: 45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRD - Reformulation Sweetcorn 10905989_IFF_SUA</td>\n",
       "      <td>46172</td>\n",
       "      <td>Cheetos Shots Sweetcorn / Cerezza TV Sweetcorn</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>Reformulation of SweetCorn 10905989 seasoning ...</td>\n",
       "      <td>not provided</td>\n",
       "      <td>Zeynep Yolcu 0090 850 279 40 00</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRD - Reformulation - SYM Peanut Flavour_Turkey</td>\n",
       "      <td>31989</td>\n",
       "      <td>Cheetos Peanut</td>\n",
       "      <td>Productivity</td>\n",
       "      <td>Reformulation of Peanut 355722 seasoning to dr...</td>\n",
       "      <td>not provided</td>\n",
       "      <td>Zeynep Yolcu 0090 850 279 40 00</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>needs to be decided as to what value</td>\n",
       "      <td>needs to be decided as to what value</td>\n",
       "      <td>needs to be decided as to what value</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chipniks Commercialization at Awesome Snacks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simba Chipniks</td>\n",
       "      <td>Capacity Expansion</td>\n",
       "      <td>Leveraging on Awesome Snacks extra fryer capac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sherwin Tlhoaele</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crisp-mas 2018</td>\n",
       "      <td>50178</td>\n",
       "      <td>Walkers LTO christmas</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>5 Christmas LTO's 3 x NPD 2 x L&amp;S</td>\n",
       "      <td>christmas</td>\n",
       "      <td>Steve Wilson</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Simba Shapes Next Gen</td>\n",
       "      <td>36559</td>\n",
       "      <td>Simba Shapes - Playz, Simba Shapes Twistz</td>\n",
       "      <td>Breakthrough - South Africa &amp; Nigeria</td>\n",
       "      <td>Purchase an existing Pellet spec used by Pepsi...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Dina Atoyan (+7(495)9370550 (*63402) )</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          projName accolNumber  \\\n",
       "0   PRD - Reformulation Sweetcorn 10905989_IFF_SUA       46172   \n",
       "1  PRD - Reformulation - SYM Peanut Flavour_Turkey       31989   \n",
       "2     Chipniks Commercialization at Awesome Snacks         NaN   \n",
       "3                                   Crisp-mas 2018       50178   \n",
       "4                            Simba Shapes Next Gen       36559   \n",
       "\n",
       "                                     PDA_projName  \\\n",
       "0  Cheetos Shots Sweetcorn / Cerezza TV Sweetcorn   \n",
       "1                                  Cheetos Peanut   \n",
       "2                                 Simba Chipniks    \n",
       "3                           Walkers LTO christmas   \n",
       "4       Simba Shapes - Playz, Simba Shapes Twistz   \n",
       "\n",
       "                                projType  \\\n",
       "0                           Productivity   \n",
       "1                           Productivity   \n",
       "2                     Capacity Expansion   \n",
       "3                                Refresh   \n",
       "4  Breakthrough - South Africa & Nigeria   \n",
       "\n",
       "                                            projDesc formulaNumber  \\\n",
       "0  Reformulation of SweetCorn 10905989 seasoning ...  not provided   \n",
       "1  Reformulation of Peanut 355722 seasoning to dr...  not provided   \n",
       "2  Leveraging on Awesome Snacks extra fryer capac...           NaN   \n",
       "3                  5 Christmas LTO's 3 x NPD 2 x L&S     christmas   \n",
       "4  Purchase an existing Pellet spec used by Pepsi...       unknown   \n",
       "\n",
       "                                    owner sector  Unnamed: 8  Unnamed: 9  \\\n",
       "0         Zeynep Yolcu 0090 850 279 40 00   ESSA         NaN         NaN   \n",
       "1         Zeynep Yolcu 0090 850 279 40 00   ESSA         NaN         NaN   \n",
       "2                        Sherwin Tlhoaele   ESSA         NaN         NaN   \n",
       "3                            Steve Wilson   ESSA         NaN         NaN   \n",
       "4  Dina Atoyan (+7(495)9370550 (*63402) )   ESSA         NaN         NaN   \n",
       "\n",
       "      ...      baseProduct prodModifications newIngradient approvedPackage  \\\n",
       "0     ...               No                No           Yes              No   \n",
       "1     ...               No                No           Yes              No   \n",
       "2     ...               No               Yes            No              No   \n",
       "3     ...               No                No            No              No   \n",
       "4     ...              Yes               Yes           Yes              No   \n",
       "\n",
       "                      potentialMicrobial  \\\n",
       "0                                    Yes   \n",
       "1  needs to be decided as to what value    \n",
       "2                                     No   \n",
       "3                                     No   \n",
       "4                                     No   \n",
       "\n",
       "                  cross_contactAllergens  \\\n",
       "0                                    Yes   \n",
       "1  needs to be decided as to what value    \n",
       "2                                    Yes   \n",
       "3                                    Yes   \n",
       "4                                    Yes   \n",
       "\n",
       "                             chokeHazard operationalAllergen Unnamed: 44  \\\n",
       "0                                     No                 Yes         NaN   \n",
       "1  needs to be decided as to what value                  Yes         NaN   \n",
       "2                                     No                 NaN         NaN   \n",
       "3                                     No                 Yes         NaN   \n",
       "4                                    Yes                 Yes         NaN   \n",
       "\n",
       "   Unnamed: 45  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "exclude = set(string.punctuation) \n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "#newStopWords = []\n",
    "#stop_words.extend(newStopWords)\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # tokenize document\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each word\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # convert to lower case\n",
    "    lower_tokens = [w.lower() for w in tokens]\n",
    "    #remove spaces\n",
    "    stripped = [w.strip() for w in lower_tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in words if token not in stop_words]\n",
    "    #apply Stemming\n",
    "    #stemmed = [porter.stem(word) for word in filtered_tokens]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_projDesc = normalize_corpus(df['projDesc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reformulation sweetcorn seasoning drive seasoning cost per kg reduction deliver productivity product qualification technical match action standart change base product production seasoning changing'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_projDesc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reformulation peanut seasoning drive seasoning cost per kg reduction deliver productivity product qualification technical match action standart change base product production seasoning changing'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_projDesc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\103467\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['potentialMicrobial'][1]='Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[]\n",
    "for i in range(len(df)):\n",
    "    if (df['potentialMicrobial'][i] =='Yes'):\n",
    "        train_label.append(1)\n",
    "    else:\n",
    "        train_label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features -  ignored for now, as we do not have any correlation established\n",
    "    #selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    #selector.fit(x_train, train_labels)\n",
    "    #x_train = selector.transform(x_train)\n",
    "    #x_val = selector.transform(x_val)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\103467\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# Vectorize texts.\n",
    "x_train, x_test = ngram_vectorize(norm_projDesc, train_label, [\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 67)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 10)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dense = x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14742255, 0.14742255, 0.        , 0.14742255, 0.14742255,\n",
       "       0.        , 0.12897731, 0.14742255, 0.14742255, 0.        ,\n",
       "       0.14742255, 0.14742255, 0.        , 0.        , 0.        ,\n",
       "       0.14742255, 0.14742255, 0.14742255, 0.14742255, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.14742255, 0.14742255, 0.        , 0.        ,\n",
       "       0.14742255, 0.14742255, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.12897731, 0.14742255, 0.        , 0.        ,\n",
       "       0.25795463, 0.14742255, 0.14742255, 0.12897731, 0.14742255,\n",
       "       0.14742255, 0.14742255, 0.        , 0.12897731, 0.14742255,\n",
       "       0.14742255, 0.14742255, 0.14742255, 0.        , 0.        ,\n",
       "       0.38693193, 0.14742255, 0.14742255, 0.14742255, 0.        ,\n",
       "       0.14742255, 0.14742255, 0.14742255, 0.14742255, 0.        ,\n",
       "       0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dense[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components = 2\n",
    "whiten = False\n",
    "random_state = 42\n",
    "svd_solver=\"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components,svd_solver=svd_solver,whiten=whiten, random_state=42)\n",
    "X_pca = pca.fit_transform(X_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

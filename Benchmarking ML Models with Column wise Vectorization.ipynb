{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reads text data from data extract created from FSHA Forms and runs predictive Models to predict the value 'Are there any inherent or cross contact allergens or intolerants?' , based on the Input Data\n",
    "# It does vectorization of each Column and concatenates these Vectors to create a final Feature Vector and fits multiple ML Models, including Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary modules\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from scipy import signal\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name and other important parameters like ngram_range set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These parameters will be input from command line\n",
    "ngram_range_inp=(1,2)\n",
    "filename = \"C:/Pepsico/FSHA RPA - 23 July 2019.xlsm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define reusable modular method for Text Normalization (removal of stopwords, changing to lower case, removal of punctuation etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) \n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['from','dtype','object']\n",
    "stop_words.extend(newStopWords)\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # tokenize document\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each word\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # convert to lower case\n",
    "    lower_tokens = [w.lower() for w in tokens]\n",
    "    #remove spaces\n",
    "    stripped = [w.strip() for w in lower_tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in words if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "#normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    #corpus = str(corpus)\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_doc(corpus):\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    #corpus = str(corpus)\n",
    "    for doc in corpus:\n",
    "\t# split into tokens by white space\n",
    "        doc=str(doc)\n",
    "        doc = doc.replace('\\n',' ')\n",
    "        tokens = doc.split()\n",
    "        # prepare regex for char filtering\n",
    "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        # remove punctuation from each word\n",
    "        tokens = [re_punc.sub('', w) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # remove nn from each word\n",
    "        tokens = [re.sub('nn',' ',word) for word in tokens]\n",
    "        tokens = ' '.join(tokens)\n",
    "        normalized_corpus.append(tokens)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data extract file (tabular format with Input data(X) and target(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsha_data = pd.read_excel(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fsha_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>projName</th>\n",
       "      <th>accolNumber</th>\n",
       "      <th>PDA_projName</th>\n",
       "      <th>projType</th>\n",
       "      <th>projDesc</th>\n",
       "      <th>formulaNumber</th>\n",
       "      <th>owner</th>\n",
       "      <th>sector</th>\n",
       "      <th>center</th>\n",
       "      <th>...</th>\n",
       "      <th>Table1_Row6_Molluscs</th>\n",
       "      <th>Table1_Row6_Mustard</th>\n",
       "      <th>Table1_Row6_Sesame Seeds</th>\n",
       "      <th>Table1_Row6_Sulphites</th>\n",
       "      <th>Table1_Row1_Moulluscs</th>\n",
       "      <th>Table1_Row2_Moulluscs</th>\n",
       "      <th>Table1_Row3_Moulluscs</th>\n",
       "      <th>Table1_Row4_Moulluscs</th>\n",
       "      <th>Table1_Row5_Moulluscs</th>\n",
       "      <th>Table1_Row6_Moulluscs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#46565 FSHA 5.4.1Star Project G3 v2 + FS input...</td>\n",
       "      <td>S-T3-Star-• POL Star Puff (Chrupki) quality –POL</td>\n",
       "      <td>46565</td>\n",
       "      <td>1SKU Star Puffs Onion\\n2 SKU Star Puffs Cheese...</td>\n",
       "      <td>Brand Refresh</td>\n",
       "      <td>Star Puffs Cheese &amp; STar Hyper Cheese\\nSeasoni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weronika Baranowska</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Warsaw</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#53697 FSHA HT Baguette 4 Cheese UA 2.07.19.xlsm</td>\n",
       "      <td>HT Baguette Four Cheese Flavor</td>\n",
       "      <td>53697</td>\n",
       "      <td>Hrusteam Baguette</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Launch new seasoning 4 Cheese NL-502-352-9 on ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anna Nikonova</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#57686 FSHA 5.4.1 Red Caviar  Azov.xlsm</td>\n",
       "      <td>Lay's Caviar IO 2019 RUS Asov</td>\n",
       "      <td>57686</td>\n",
       "      <td>Lay's Red Caviar</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Idea is to launch I&amp;O flavour under New Year p...</td>\n",
       "      <td>Not provided</td>\n",
       "      <td>Evgeniy Shklovskiy +79163257848</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#57686 FSHA 5.4.1 Red Caviar Kashira.xlsm</td>\n",
       "      <td>Lay's Caviar IO 2019 RUS Kashira</td>\n",
       "      <td>57686</td>\n",
       "      <td>Lay's Red Caviar</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Idea is to launch I&amp;O flavour under New Year p...</td>\n",
       "      <td>Not provided</td>\n",
       "      <td>Evgeniy Shklovskiy +79163257848</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53354-FSHA-In Process 13.12.18.xlsm</td>\n",
       "      <td>Soft and Mild Iberia 2019</td>\n",
       "      <td>53354</td>\n",
       "      <td>Cheetos Palomito</td>\n",
       "      <td>Re Launch</td>\n",
       "      <td>Re Launch of Cheetos Palomitos, Soft Extruded ...</td>\n",
       "      <td>CP2019</td>\n",
       "      <td>David Labrado 07770646572</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Beaumont Park</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  #46565 FSHA 5.4.1Star Project G3 v2 + FS input...   \n",
       "1   #53697 FSHA HT Baguette 4 Cheese UA 2.07.19.xlsm   \n",
       "2            #57686 FSHA 5.4.1 Red Caviar  Azov.xlsm   \n",
       "3          #57686 FSHA 5.4.1 Red Caviar Kashira.xlsm   \n",
       "4                53354-FSHA-In Process 13.12.18.xlsm   \n",
       "\n",
       "                                            projName accolNumber  \\\n",
       "0  S-T3-Star-• POL Star Puff (Chrupki) quality –POL        46565   \n",
       "1                     HT Baguette Four Cheese Flavor       53697   \n",
       "2                      Lay's Caviar IO 2019 RUS Asov       57686   \n",
       "3                   Lay's Caviar IO 2019 RUS Kashira       57686   \n",
       "4                          Soft and Mild Iberia 2019       53354   \n",
       "\n",
       "                                        PDA_projName       projType  \\\n",
       "0  1SKU Star Puffs Onion\\n2 SKU Star Puffs Cheese...  Brand Refresh   \n",
       "1                                  Hrusteam Baguette        Refresh   \n",
       "2                                  Lay's Red Caviar         Refresh   \n",
       "3                                  Lay's Red Caviar         Refresh   \n",
       "4                                   Cheetos Palomito      Re Launch   \n",
       "\n",
       "                                            projDesc formulaNumber  \\\n",
       "0  Star Puffs Cheese & STar Hyper Cheese\\nSeasoni...           NaN   \n",
       "1  Launch new seasoning 4 Cheese NL-502-352-9 on ...           NaN   \n",
       "2  Idea is to launch I&O flavour under New Year p...  Not provided   \n",
       "3  Idea is to launch I&O flavour under New Year p...  Not provided   \n",
       "4  Re Launch of Cheetos Palomitos, Soft Extruded ...        CP2019   \n",
       "\n",
       "                             owner sector         center  \\\n",
       "0              Weronika Baranowska   ESSA         Warsaw   \n",
       "1                    Anna Nikonova   ESSA         Moscow   \n",
       "2  Evgeniy Shklovskiy +79163257848   ESSA         Moscow   \n",
       "3  Evgeniy Shklovskiy +79163257848   ESSA         Moscow   \n",
       "4        David Labrado 07770646572   ESSA  Beaumont Park   \n",
       "\n",
       "           ...          Table1_Row6_Molluscs Table1_Row6_Mustard  \\\n",
       "0          ...                           0.0                   1   \n",
       "1          ...                           0.0                   1   \n",
       "2          ...                           0.0                   0   \n",
       "3          ...                           0.0                   0   \n",
       "4          ...                           0.0                   0   \n",
       "\n",
       "  Table1_Row6_Sesame Seeds Table1_Row6_Sulphites Table1_Row1_Moulluscs  \\\n",
       "0                        0                     0                   NaN   \n",
       "1                        0                     0                   NaN   \n",
       "2                        0                     0                   NaN   \n",
       "3                        0                     0                   NaN   \n",
       "4                        0                     0                   NaN   \n",
       "\n",
       "  Table1_Row2_Moulluscs Table1_Row3_Moulluscs Table1_Row4_Moulluscs  \\\n",
       "0                   NaN                   NaN                   NaN   \n",
       "1                   NaN                   NaN                   NaN   \n",
       "2                   NaN                   NaN                   NaN   \n",
       "3                   NaN                   NaN                   NaN   \n",
       "4                   NaN                   NaN                   NaN   \n",
       "\n",
       "  Table1_Row5_Moulluscs Table1_Row6_Moulluscs  \n",
       "0                   NaN                   NaN  \n",
       "1                   NaN                   NaN  \n",
       "2                   NaN                   NaN  \n",
       "3                   NaN                   NaN  \n",
       "4                   NaN                   NaN  \n",
       "\n",
       "[5 rows x 136 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsha_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Analysis select the Features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selecting set of columns as Features\n",
    "features_df=fsha_data[['preservatives', 'pH', 'waterActivity', 'packaging','otherFSA',\n",
    "            'prodStorageDist', 'foodSafetyProdClaims','targetMarket','allergens','newIngredient','allergensLabeledIMAF']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing values in features with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define reusable code to Vectorize Text column (ex: Allergens) using TF-IDF Vectorizer, after doing Text data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorization of text data using TF-IDF Vectorizer\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "#NGRAM_RANGE \n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "#TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels,ngram_range):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': ngram_range,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['preservatives', 'pH', 'waterActivity', 'packaging', 'otherFSA',\n",
       "       'prodStorageDist', 'foodSafetyProdClaims', 'targetMarket', 'allergens',\n",
       "       'newIngredient', 'allergensLabeledIMAF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize each column , by cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['norm_preservatives'] = normalize_corpus(train_df['preservatives'])\n",
    "train_df['norm_pH'] = normalize_corpus(train_df['pH'])\n",
    "train_df['norm_waterActivity'] = clean_doc(train_df['waterActivity'])\n",
    "train_df['norm_packaging'] = normalize_corpus(train_df['packaging'])\n",
    "train_df['norm_otherFSA'] = normalize_corpus(train_df['otherFSA'])\n",
    "train_df['norm_prodStorageDist'] = normalize_corpus(train_df['prodStorageDist'])\n",
    "train_df['norm_foodSafety_prodClaims'] = normalize_corpus(train_df['foodSafetyProdClaims'])\n",
    "train_df['norm_targetMarket'] = normalize_corpus(train_df['targetMarket'])\n",
    "train_df['norm_newIngredient'] = normalize_corpus(train_df['newIngredient'])\n",
    "train_df['norm_allergens'] = normalize_corpus(train_df['allergens'])\n",
    "train_df['norm_allergens_M']=normalize_corpus(train_df['allergensLabeledIMAF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['allergens',\n",
    " 'newIngredient',\n",
    " 'pH',\n",
    " 'prodStorageDist',\n",
    " 'waterActivity',\n",
    " 'packaging',\n",
    " 'preservatives',\n",
    " 'otherFSA',\n",
    " 'foodSafetyProdClaims',\n",
    " 'targetMarket','allergensLabeledIMAF'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['norm_preservatives', 'norm_pH', 'norm_waterActivity', 'norm_packaging',\n",
       "       'norm_otherFSA', 'norm_prodStorageDist', 'norm_foodSafety_prodClaims',\n",
       "       'norm_targetMarket', 'norm_newIngredient', 'norm_allergens',\n",
       "       'norm_allergens_M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    allergens seasonings cq69 vegetable blend mcco...\n",
      "1    no new allergen line seasonings 4 cheese conta...\n",
      "2    inherent milk lactose wheat gluten fish compon...\n",
      "3    inherent inherent milk lactose wheat gluten fi...\n",
      "4                                       milk seasoning\n",
      "Name: norm_allergens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['norm_allergens'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the labels in the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selecting set of columns for labeling\n",
    "\n",
    "#ph value label\n",
    "train_df['pH_label']=train_df['norm_pH'].apply(lambda x:re.findall(r'[0-9]+', str(x)) if bool(re.search(r'\\d', str(x))) else str(0))\n",
    "def ph_process(x):\n",
    "    if(len(x)>0):\n",
    "        x = str(x[0])\n",
    "    else:\n",
    "        x = str(x)\n",
    "    return x\n",
    "train_df['pH_label']=train_df['pH_label'].apply(lambda x:ph_process(x))    \n",
    "\n",
    "#'prod_storageDist'column label\n",
    "def prod_storageDist(x):\n",
    "    x=str(x.lower())\n",
    "    if x.__contains__('sun'):\n",
    "            return 'Keep away from sun'\n",
    "    elif x.__contains__('ambient')  :\n",
    "              return 'ambient'\n",
    "    elif x.__contains__('rte'):\n",
    "        return 'RTE'\n",
    "    else:\n",
    "        return 'NA'\n",
    "train_df['prod_storageDist_label']=train_df['norm_prodStorageDist'].apply(lambda x:prod_storageDist(x))                                                              \n",
    "\n",
    "#water activity\n",
    "def water_activity(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if x.__contains__('low'):\n",
    "        return 'low'\n",
    "    elif x.__contains__('max'):\n",
    "        return 'max'\n",
    "    elif x==str(np.nan):\n",
    "        return 'NA'\n",
    "    elif bool(re.search('none|n/a|na',x)):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return(x)\n",
    "train_df['waterActivity_label']=train_df['norm_waterActivity'].apply(lambda x:water_activity(x))                                                              \n",
    " \n",
    "#packaging'\n",
    "def packaging(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if x.__contains__('no') and x.__contains__('nitrogen'):\n",
    "        return 'no nitrogen'\n",
    "    elif x.__contains__('not') and x.__contains__('n2'):\n",
    "        return 'no nitrogen'\n",
    "    elif x.__contains__('nitrogen') and not x.__contains__('no'):\n",
    "        return 'nitrogen'\n",
    "    elif x.__contains__('atmosphere'):\n",
    "        return 'atmosphere'\n",
    "    elif x==str(np.nan):\n",
    "        return 'NA'\n",
    "    elif bool(re.search('none|n/a|na',x)):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return(x)\n",
    "    \n",
    "train_df['packaging_label']=train_df['norm_packaging'].apply(lambda x: packaging(x))  \n",
    "  \n",
    "#'preservatives'\n",
    "def preservatives(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if x.__contains__('sodium'):\n",
    "        return 'Not used as preservatives'\n",
    "    elif x.__contains__('not') and x.__contains__('seasoning'):\n",
    "        return 'No Seasoning'\n",
    "    elif bool(re.search('topping|seasoning',x)):\n",
    "        return 'Used in seasoning'\n",
    "    else:\n",
    "        return ('NA')\n",
    "    \n",
    "train_df['preservatives_label']=train_df['norm_preservatives'].apply(lambda x:preservatives(x))\n",
    "\n",
    "#' otherFSA'\n",
    "def otherFSA(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if bool(re.search('moisture',x)):\n",
    "        return 'Moisture'\n",
    "    else:\n",
    "        return 'NA'\n",
    "train_df['otherFSA_label']=train_df['norm_otherFSA'].apply(lambda x:otherFSA(x))\n",
    "\n",
    "\n",
    "def foodsafety_prodclaims(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if bool(re.search('^claim',x)):\n",
    "        return 'Claims Made'\n",
    "    elif x.__contains__('no claims'):\n",
    "        return 'No Claims Made'\n",
    "    elif x.__contains__('allergen'):\n",
    "        return 'Allergen'\n",
    "    elif bool(re.search('none|n/a|na',x)):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "train_df['foodSafety_prodClaims_label']=train_df['norm_foodSafety_prodClaims'].apply(lambda x:foodsafety_prodclaims(x))  \n",
    "\n",
    "#' targetMarket'\n",
    "def targetMarket(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if (x.__contains__('choking') or x.__contains__('choke')) and x.__contains__('children'):\n",
    "        return 'Choking hazard for children'\n",
    "    elif x.__contains__('allerg'):\n",
    "        return 'Allergy'\n",
    "    elif x.__contains__('no') and x.__contains__('change'):\n",
    "        return 'no'\n",
    "    else:\n",
    "        return(x)\n",
    "train_df['targetMarket_label']=train_df['norm_targetMarket'].apply(lambda x:targetMarket(x))\n",
    "\n",
    "train_df=train_df.drop(['norm_preservatives', 'norm_pH', 'norm_waterActivity', 'norm_packaging','norm_otherFSA',\n",
    "            'norm_prodStorageDist', 'norm_foodSafety_prodClaims','norm_targetMarket'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_newIngredient</th>\n",
       "      <th>norm_allergens</th>\n",
       "      <th>norm_allergens_M</th>\n",
       "      <th>pH_label</th>\n",
       "      <th>prod_storageDist_label</th>\n",
       "      <th>waterActivity_label</th>\n",
       "      <th>packaging_label</th>\n",
       "      <th>preservatives_label</th>\n",
       "      <th>otherFSA_label</th>\n",
       "      <th>foodSafety_prodClaims_label</th>\n",
       "      <th>targetMarket_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>allergens seasonings cq69 vegetable blend mcco...</td>\n",
       "      <td>milk lactose milk lactose</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep away from sun</td>\n",
       "      <td>low</td>\n",
       "      <td>atmosphere</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Claims Made</td>\n",
       "      <td>Allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>no new allergen line seasonings 4 cheese conta...</td>\n",
       "      <td>milk lactose sulphites 1ppm gluten 3ppm</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>low</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>inherent milk lactose wheat gluten fish compon...</td>\n",
       "      <td>milk lactose wheat gluten 3ppm sulphites 1ppm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>low</td>\n",
       "      <td>standard</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>inherent inherent milk lactose wheat gluten fi...</td>\n",
       "      <td>milk lactose wheat gluten 3ppm sulphites 1ppm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>low</td>\n",
       "      <td>standard</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>Allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>milk seasoning</td>\n",
       "      <td>na na milk</td>\n",
       "      <td>0</td>\n",
       "      <td>Keep away from sun</td>\n",
       "      <td>low</td>\n",
       "      <td>window clear film</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>gluten free</td>\n",
       "      <td>kids 410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  norm_newIngredient                                     norm_allergens  \\\n",
       "0                yes  allergens seasonings cq69 vegetable blend mcco...   \n",
       "1                yes  no new allergen line seasonings 4 cheese conta...   \n",
       "2                 no  inherent milk lactose wheat gluten fish compon...   \n",
       "3                 no  inherent inherent milk lactose wheat gluten fi...   \n",
       "4                 no                                     milk seasoning   \n",
       "\n",
       "                                    norm_allergens_M pH_label  \\\n",
       "0                          milk lactose milk lactose        0   \n",
       "1            milk lactose sulphites 1ppm gluten 3ppm        0   \n",
       "2  milk lactose wheat gluten 3ppm sulphites 1ppm ...        0   \n",
       "3  milk lactose wheat gluten 3ppm sulphites 1ppm ...        0   \n",
       "4                                         na na milk        0   \n",
       "\n",
       "  prod_storageDist_label waterActivity_label    packaging_label  \\\n",
       "0     Keep away from sun                 low         atmosphere   \n",
       "1                     NA                 low                 NA   \n",
       "2                     NA                 low           standard   \n",
       "3                     NA                 low           standard   \n",
       "4     Keep away from sun                 low  window clear film   \n",
       "\n",
       "  preservatives_label otherFSA_label foodSafety_prodClaims_label  \\\n",
       "0                  NA             NA                 Claims Made   \n",
       "1                  NA             NA                          NA   \n",
       "2                  NA             NA                          NA   \n",
       "3                  NA             NA                          NA   \n",
       "4                  NA             NA                 gluten free   \n",
       "\n",
       "  targetMarket_label  \n",
       "0            Allergy  \n",
       "1                 no  \n",
       "2            Allergy  \n",
       "3            Allergy  \n",
       "4           kids 410  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding of categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,train_df.shape[1]):\n",
    "    if train_df.columns[i] != 'norm_allergens' :\n",
    "        if train_df.columns[i] != 'norm_allergens_M':\n",
    "        \n",
    "            df=pd.get_dummies(train_df[train_df.columns[i]])\n",
    "            df.columns = [train_df.columns[i]+\"_\"+k for k in df.columns]\n",
    "            if i ==0 :\n",
    "                newdf = df\n",
    "            else:\n",
    "                newdf = pd.concat([newdf,df],axis=1)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(newdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf['norm_allergens']= train_df.norm_allergens\n",
    "newdf['norm_allergens_M']=train_df.norm_allergens_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df=newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['norm_newIngredient_no', 'norm_newIngredient_yes', 'pH_label_0',\n",
       "       'pH_label_3', 'prod_storageDist_label_Keep away from sun',\n",
       "       'prod_storageDist_label_NA', 'prod_storageDist_label_ambient',\n",
       "       'waterActivity_label_NA', 'waterActivity_label_as per current pc',\n",
       "       'waterActivity_label_low', 'waterActivity_label_max',\n",
       "       'waterActivity_label_not determined',\n",
       "       'waterActivity_label_not specified pd', 'packaging_label_NA',\n",
       "       'packaging_label_atmosphere', 'packaging_label_high barrier film',\n",
       "       'packaging_label_nitrogen', 'packaging_label_no different current tc',\n",
       "       'packaging_label_no nitrogen', 'packaging_label_not applicable',\n",
       "       'packaging_label_not confirmed pd', 'packaging_label_not provided',\n",
       "       'packaging_label_not required', 'packaging_label_standard',\n",
       "       'packaging_label_unknown', 'packaging_label_window clear film',\n",
       "       'packaging_label_wip bags part mix', 'preservatives_label_NA',\n",
       "       'preservatives_label_No Seasoning',\n",
       "       'preservatives_label_Not used as preservatives',\n",
       "       'otherFSA_label_Moisture', 'otherFSA_label_NA',\n",
       "       'foodSafety_prodClaims_label_50 less fat comparing fried potato chips',\n",
       "       'foodSafety_prodClaims_label_Claims Made',\n",
       "       'foodSafety_prodClaims_label_NA',\n",
       "       'foodSafety_prodClaims_label_No Claims Made',\n",
       "       'foodSafety_prodClaims_label_gluten free',\n",
       "       'foodSafety_prodClaims_label_no',\n",
       "       'foodSafety_prodClaims_label_no different current tc',\n",
       "       'foodSafety_prodClaims_label_no food safety related claims',\n",
       "       'foodSafety_prodClaims_label_no food safety related product claims',\n",
       "       'foodSafety_prodClaims_label_no preservatives',\n",
       "       'foodSafety_prodClaims_label_not applicable',\n",
       "       'foodSafety_prodClaims_label_product may contain peanut',\n",
       "       'foodSafety_prodClaims_label_tbc', 'targetMarket_label_Allergy',\n",
       "       'targetMarket_label_Choking hazard for children',\n",
       "       'targetMarket_label_kids 410', 'targetMarket_label_na',\n",
       "       'targetMarket_label_no', 'targetMarket_label_no vulnerable groups',\n",
       "       'targetMarket_label_none', 'targetMarket_label_not',\n",
       "       'targetMarket_label_not suitable people vulnerable gluten milk deretives',\n",
       "       'norm_allergens', 'norm_allergens_M'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize the target (1/0 for Yes/No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics \n",
    "\n",
    "def impute_target(fsha_data,targetName):\n",
    "    train_y=[]\n",
    "    for i in range (len(fsha_data)):\n",
    "        if str(fsha_data[targetName][i]).strip().lower() =='yes':\n",
    "            train_y.append(1)\n",
    "        elif str(fsha_data[targetName][i]).strip().lower() =='no':\n",
    "            train_y.append(0)\n",
    "        else:\n",
    "            train_y.append(-1)\n",
    "               \n",
    "    mode_y = statistics.mode(train_y)\n",
    "\n",
    "    for i in range (len(fsha_data)):\n",
    "        if train_y[i]==-1:\n",
    "            train_y[i] = mode_y\n",
    "            \n",
    "    return train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target (Yes/No choice) in PDAF are converted to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_crosscontact = impute_target(fsha_data,\"crossContactAllergens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['target']=train_y_crosscontact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    164\n",
       "0      3\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train_df['target']\n",
    "y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform n-gram vectorization and PCA on text data, columnwise, and concatenate with categorical one-hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(train_df,y):    \n",
    "    train_labels = y\n",
    "    x_ngram_allergens = ngram_vectorize(train_df['norm_allergens'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_allergens_m = ngram_vectorize(train_df['norm_allergens_M'], train_labels,n_gram_range).toarray()\n",
    "    train_df = train_df.drop(['norm_allergens','norm_allergens_M'],axis=1)\n",
    "    pca = PCA(n_components=n_components,svd_solver=svd_solver,whiten=whiten, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x_ngram_allergens = scaler.fit_transform(x_ngram_allergens)\n",
    "    x_pca_allergens = pca.fit_transform(x_ngram_allergens)\n",
    "    x_pca_allergens_m = pca.fit_transform(x_ngram_allergens_m)\n",
    "    print(x_pca_allergens.shape)\n",
    "    print(x_pca_allergens_m.shape)\n",
    "    print(train_df.shape)\n",
    "    x_train = np.concatenate((train_df,x_pca_allergens,x_pca_allergens_m),axis=1)\n",
    "    print(x_train.shape)\n",
    "    return x_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 2)\n",
      "(167, 2)\n",
      "(167, 55)\n",
      "(167, 59)\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (1,2)\n",
    "n_components = 2\n",
    "whiten = False\n",
    "random_state = 42\n",
    "svd_solver=\"full\"\n",
    "x_train = preprocess_text(train_df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = x_train,train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 59)\n",
      "(34, 59)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.columns = ['norm_newIngredient_no', 'norm_newIngredient_yes', 'pH_label_0',\n",
    "       'pH_label_3', 'prod_storageDist_label_Keep away from sun',\n",
    "       'prod_storageDist_label_NA', 'prod_storageDist_label_ambient',\n",
    "       'waterActivity_label_NA', 'waterActivity_label_as per current pc',\n",
    "       'waterActivity_label_low', 'waterActivity_label_max',\n",
    "       'waterActivity_label_not determined',\n",
    "       'waterActivity_label_not specified pd', 'packaging_label_NA',\n",
    "       'packaging_label_atmosphere', 'packaging_label_high barrier film',\n",
    "       'packaging_label_nitrogen', 'packaging_label_no different current tc',\n",
    "       'packaging_label_no nitrogen', 'packaging_label_not applicable',\n",
    "       'packaging_label_not confirmed pd', 'packaging_label_not provided',\n",
    "       'packaging_label_not required', 'packaging_label_standard',\n",
    "       'packaging_label_unknown', 'packaging_label_window clear film',\n",
    "       'packaging_label_wip bags part mix', 'preservatives_label_NA',\n",
    "       'preservatives_label_No Seasoning',\n",
    "       'preservatives_label_Not used as preservatives',\n",
    "       'otherFSA_label_Moisture', 'otherFSA_label_NA',\n",
    "       'foodSafety_prodClaims_label_50 less fat comparing fried potato chips',\n",
    "       'foodSafety_prodClaims_label_Claims Made',\n",
    "       'foodSafety_prodClaims_label_NA',\n",
    "       'foodSafety_prodClaims_label_No Claims Made',\n",
    "       'foodSafety_prodClaims_label_gluten free',\n",
    "       'foodSafety_prodClaims_label_no',\n",
    "       'foodSafety_prodClaims_label_no different current tc',\n",
    "       'foodSafety_prodClaims_label_no food safety related claims',\n",
    "       'foodSafety_prodClaims_label_no food safety related product claims',\n",
    "       'foodSafety_prodClaims_label_no preservatives',\n",
    "       'foodSafety_prodClaims_label_not applicable',\n",
    "       'foodSafety_prodClaims_label_product may contain peanut',\n",
    "       'foodSafety_prodClaims_label_tbc', 'targetMarket_label_Allergy',\n",
    "       'targetMarket_label_Choking hazard for children',\n",
    "       'targetMarket_label_kids 410', 'targetMarket_label_na',\n",
    "       'targetMarket_label_no', 'targetMarket_label_no vulnerable groups',\n",
    "       'targetMarket_label_none', 'targetMarket_label_not',\n",
    "       'targetMarket_label_not suitable people vulnerable gluten milk deretives','PCA_allergens_1','PCA_allergens_2','PCA_allergens_M_1','PCA_allergens_M_2','Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample the crossContact Allergens data, since the data is highly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "not_crosscontact = X[X.target==0]\n",
    "crosscontact = X[X.target==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "print(len(not_crosscontact))\n",
    "print(len(crosscontact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "not_crosscontact_upsampled = resample(not_crosscontact,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(crosscontact), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([not_crosscontact_upsampled, crosscontact])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    131\n",
       "0.0    131\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = upsampled.target\n",
    "X_train = upsampled.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 59)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([236, 236, 236, ..., 261, 261, 261], dtype=int64),\n",
       " array([ 0,  1,  2, ..., 56, 57, 58], dtype=int64))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.fillna( method ='ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define method to evaluate Machine Learning models with the X and y vectors created above, and check the effectiveness of each. Also store the results in array to be plotted in graph for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(len(X_train))\n",
    "    \n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    print(\"model name:\"+clf_descr)\n",
    "  \n",
    "    a = datetime.now()\n",
    "    \n",
    "    if clf_descr.__contains__('tensorflow'):\n",
    "        history = clf.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(X_test, y_test),\n",
    "            verbose=2, \n",
    "            batch_size=batch_size)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "    b = datetime.now()\n",
    "    c = a-b\n",
    "    train_time = c.microseconds\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    pred_train = clf.predict(X_train)\n",
    " \n",
    "    if clf_descr.__contains__('tensorflow'):\n",
    "        for i in range (len(pred)):\n",
    "            if (pred[i]>=0.3):\n",
    "                pred[i]=1\n",
    "            else:\n",
    "                pred[i]=0\n",
    "        for i in range (len(pred_train)):        \n",
    "            if (pred_train[i]>=0.3):\n",
    "                pred_train[i]=1\n",
    "            else:\n",
    "                pred_train[i]=0\n",
    "    \n",
    "    f1_score = metrics.f1_score(y_test, pred)\n",
    "    print(\"f1_score:   %0.3f\" % f1_score )\n",
    "    \n",
    "    f1_score_train = metrics.f1_score(y_train, pred_train)\n",
    "    print(\"f1_score_train:   %0.3f\" % f1_score_train )\n",
    "    \n",
    "    print(\"classification report:\")\n",
    "    print(classification_report(y_test, pred))\n",
    "    \n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "    \n",
    "    return clf_descr,f1_score_train,f1_score,train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm_newIngredient_no</th>\n",
       "      <th>norm_newIngredient_yes</th>\n",
       "      <th>pH_label_0</th>\n",
       "      <th>pH_label_3</th>\n",
       "      <th>prod_storageDist_label_Keep away from sun</th>\n",
       "      <th>prod_storageDist_label_NA</th>\n",
       "      <th>prod_storageDist_label_ambient</th>\n",
       "      <th>waterActivity_label_NA</th>\n",
       "      <th>waterActivity_label_as per current pc</th>\n",
       "      <th>waterActivity_label_low</th>\n",
       "      <th>...</th>\n",
       "      <th>targetMarket_label_no</th>\n",
       "      <th>targetMarket_label_no vulnerable groups</th>\n",
       "      <th>targetMarket_label_none</th>\n",
       "      <th>targetMarket_label_not</th>\n",
       "      <th>targetMarket_label_not suitable people vulnerable gluten milk deretives</th>\n",
       "      <th>PCA_allergens_1</th>\n",
       "      <th>PCA_allergens_2</th>\n",
       "      <th>PCA_allergens_M_1</th>\n",
       "      <th>PCA_allergens_M_2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.622201</td>\n",
       "      <td>-3.005318</td>\n",
       "      <td>-0.075487</td>\n",
       "      <td>0.886463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.893882</td>\n",
       "      <td>-1.028835</td>\n",
       "      <td>0.435130</td>\n",
       "      <td>-0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.699354</td>\n",
       "      <td>-0.695291</td>\n",
       "      <td>0.361128</td>\n",
       "      <td>-0.099710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.904681</td>\n",
       "      <td>-0.630370</td>\n",
       "      <td>-0.082874</td>\n",
       "      <td>0.079337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.918554</td>\n",
       "      <td>-0.926693</td>\n",
       "      <td>0.373695</td>\n",
       "      <td>-0.106418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.597774</td>\n",
       "      <td>-0.643616</td>\n",
       "      <td>-0.202637</td>\n",
       "      <td>0.079399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     norm_newIngredient_no  norm_newIngredient_yes  pH_label_0  pH_label_3  \\\n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "34                     0.0                     1.0         1.0         0.0   \n",
       "35                     1.0                     0.0         1.0         0.0   \n",
       "..                     ...                     ...         ...         ...   \n",
       "128                    1.0                     0.0         1.0         0.0   \n",
       "129                    0.0                     1.0         1.0         0.0   \n",
       "130                    1.0                     0.0         1.0         0.0   \n",
       "131                    1.0                     0.0         1.0         0.0   \n",
       "134                    1.0                     0.0         1.0         0.0   \n",
       "135                    1.0                     0.0         1.0         0.0   \n",
       "136                    1.0                     0.0         1.0         0.0   \n",
       "137                    1.0                     0.0         1.0         0.0   \n",
       "138                    1.0                     0.0         1.0         0.0   \n",
       "140                    1.0                     0.0         1.0         0.0   \n",
       "141                    1.0                     0.0         1.0         0.0   \n",
       "142                    1.0                     0.0         1.0         0.0   \n",
       "143                    1.0                     0.0         1.0         0.0   \n",
       "144                    1.0                     0.0         1.0         0.0   \n",
       "145                    1.0                     0.0         1.0         0.0   \n",
       "146                    1.0                     0.0         1.0         0.0   \n",
       "147                    1.0                     0.0         1.0         0.0   \n",
       "148                    1.0                     0.0         1.0         0.0   \n",
       "149                    1.0                     0.0         1.0         0.0   \n",
       "150                    1.0                     0.0         1.0         0.0   \n",
       "151                    1.0                     0.0         1.0         0.0   \n",
       "155                    1.0                     0.0         1.0         0.0   \n",
       "156                    1.0                     0.0         1.0         0.0   \n",
       "157                    1.0                     0.0         1.0         0.0   \n",
       "159                    1.0                     0.0         1.0         0.0   \n",
       "160                    1.0                     0.0         1.0         0.0   \n",
       "161                    1.0                     0.0         1.0         0.0   \n",
       "163                    1.0                     0.0         1.0         0.0   \n",
       "164                    1.0                     0.0         1.0         0.0   \n",
       "166                    1.0                     0.0         1.0         0.0   \n",
       "\n",
       "     prod_storageDist_label_Keep away from sun  prod_storageDist_label_NA  \\\n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "34                                         1.0                        0.0   \n",
       "35                                         0.0                        0.0   \n",
       "..                                         ...                        ...   \n",
       "128                                        0.0                        0.0   \n",
       "129                                        0.0                        0.0   \n",
       "130                                        0.0                        0.0   \n",
       "131                                        0.0                        0.0   \n",
       "134                                        0.0                        0.0   \n",
       "135                                        0.0                        0.0   \n",
       "136                                        0.0                        0.0   \n",
       "137                                        0.0                        0.0   \n",
       "138                                        0.0                        0.0   \n",
       "140                                        0.0                        0.0   \n",
       "141                                        0.0                        0.0   \n",
       "142                                        0.0                        0.0   \n",
       "143                                        0.0                        0.0   \n",
       "144                                        0.0                        0.0   \n",
       "145                                        0.0                        0.0   \n",
       "146                                        0.0                        0.0   \n",
       "147                                        0.0                        0.0   \n",
       "148                                        0.0                        0.0   \n",
       "149                                        0.0                        0.0   \n",
       "150                                        0.0                        0.0   \n",
       "151                                        0.0                        0.0   \n",
       "155                                        0.0                        0.0   \n",
       "156                                        0.0                        0.0   \n",
       "157                                        0.0                        0.0   \n",
       "159                                        0.0                        0.0   \n",
       "160                                        0.0                        0.0   \n",
       "161                                        0.0                        0.0   \n",
       "163                                        0.0                        0.0   \n",
       "164                                        0.0                        0.0   \n",
       "166                                        0.0                        0.0   \n",
       "\n",
       "     prod_storageDist_label_ambient  waterActivity_label_NA  \\\n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "34                              0.0                     1.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "35                              1.0                     0.0   \n",
       "34                              0.0                     1.0   \n",
       "34                              0.0                     1.0   \n",
       "35                              1.0                     0.0   \n",
       "..                              ...                     ...   \n",
       "128                             1.0                     0.0   \n",
       "129                             1.0                     0.0   \n",
       "130                             1.0                     0.0   \n",
       "131                             1.0                     0.0   \n",
       "134                             1.0                     0.0   \n",
       "135                             1.0                     0.0   \n",
       "136                             1.0                     0.0   \n",
       "137                             1.0                     0.0   \n",
       "138                             1.0                     0.0   \n",
       "140                             1.0                     0.0   \n",
       "141                             1.0                     0.0   \n",
       "142                             1.0                     0.0   \n",
       "143                             1.0                     0.0   \n",
       "144                             1.0                     0.0   \n",
       "145                             1.0                     0.0   \n",
       "146                             1.0                     0.0   \n",
       "147                             1.0                     0.0   \n",
       "148                             1.0                     0.0   \n",
       "149                             1.0                     0.0   \n",
       "150                             1.0                     0.0   \n",
       "151                             1.0                     0.0   \n",
       "155                             1.0                     0.0   \n",
       "156                             1.0                     0.0   \n",
       "157                             1.0                     0.0   \n",
       "159                             1.0                     0.0   \n",
       "160                             1.0                     0.0   \n",
       "161                             1.0                     0.0   \n",
       "163                             1.0                     0.0   \n",
       "164                             1.0                     0.0   \n",
       "166                             1.0                     0.0   \n",
       "\n",
       "     waterActivity_label_as per current pc  waterActivity_label_low    ...     \\\n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "34                                     0.0                      0.0    ...      \n",
       "35                                     0.0                      1.0    ...      \n",
       "..                                     ...                      ...    ...      \n",
       "128                                    0.0                      1.0    ...      \n",
       "129                                    0.0                      1.0    ...      \n",
       "130                                    0.0                      1.0    ...      \n",
       "131                                    0.0                      1.0    ...      \n",
       "134                                    0.0                      1.0    ...      \n",
       "135                                    0.0                      1.0    ...      \n",
       "136                                    0.0                      1.0    ...      \n",
       "137                                    0.0                      1.0    ...      \n",
       "138                                    0.0                      1.0    ...      \n",
       "140                                    0.0                      1.0    ...      \n",
       "141                                    0.0                      1.0    ...      \n",
       "142                                    0.0                      1.0    ...      \n",
       "143                                    0.0                      1.0    ...      \n",
       "144                                    0.0                      1.0    ...      \n",
       "145                                    0.0                      1.0    ...      \n",
       "146                                    0.0                      1.0    ...      \n",
       "147                                    0.0                      1.0    ...      \n",
       "148                                    0.0                      1.0    ...      \n",
       "149                                    0.0                      1.0    ...      \n",
       "150                                    0.0                      1.0    ...      \n",
       "151                                    0.0                      1.0    ...      \n",
       "155                                    0.0                      1.0    ...      \n",
       "156                                    0.0                      1.0    ...      \n",
       "157                                    0.0                      1.0    ...      \n",
       "159                                    0.0                      1.0    ...      \n",
       "160                                    0.0                      1.0    ...      \n",
       "161                                    0.0                      1.0    ...      \n",
       "163                                    0.0                      1.0    ...      \n",
       "164                                    0.0                      1.0    ...      \n",
       "166                                    0.0                      1.0    ...      \n",
       "\n",
       "     targetMarket_label_no  targetMarket_label_no vulnerable groups  \\\n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "34                     0.0                                      0.0   \n",
       "35                     0.0                                      0.0   \n",
       "..                     ...                                      ...   \n",
       "128                    0.0                                      1.0   \n",
       "129                    1.0                                      0.0   \n",
       "130                    0.0                                      1.0   \n",
       "131                    1.0                                      0.0   \n",
       "134                    1.0                                      0.0   \n",
       "135                    1.0                                      0.0   \n",
       "136                    1.0                                      0.0   \n",
       "137                    1.0                                      0.0   \n",
       "138                    1.0                                      0.0   \n",
       "140                    1.0                                      0.0   \n",
       "141                    1.0                                      0.0   \n",
       "142                    1.0                                      0.0   \n",
       "143                    1.0                                      0.0   \n",
       "144                    1.0                                      0.0   \n",
       "145                    1.0                                      0.0   \n",
       "146                    1.0                                      0.0   \n",
       "147                    1.0                                      0.0   \n",
       "148                    1.0                                      0.0   \n",
       "149                    1.0                                      0.0   \n",
       "150                    1.0                                      0.0   \n",
       "151                    1.0                                      0.0   \n",
       "155                    1.0                                      0.0   \n",
       "156                    1.0                                      0.0   \n",
       "157                    1.0                                      0.0   \n",
       "159                    1.0                                      0.0   \n",
       "160                    1.0                                      0.0   \n",
       "161                    1.0                                      0.0   \n",
       "163                    1.0                                      0.0   \n",
       "164                    1.0                                      0.0   \n",
       "166                    1.0                                      0.0   \n",
       "\n",
       "     targetMarket_label_none  targetMarket_label_not  \\\n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "34                       0.0                     0.0   \n",
       "35                       0.0                     0.0   \n",
       "..                       ...                     ...   \n",
       "128                      0.0                     0.0   \n",
       "129                      0.0                     0.0   \n",
       "130                      0.0                     0.0   \n",
       "131                      0.0                     0.0   \n",
       "134                      0.0                     0.0   \n",
       "135                      0.0                     0.0   \n",
       "136                      0.0                     0.0   \n",
       "137                      0.0                     0.0   \n",
       "138                      0.0                     0.0   \n",
       "140                      0.0                     0.0   \n",
       "141                      0.0                     0.0   \n",
       "142                      0.0                     0.0   \n",
       "143                      0.0                     0.0   \n",
       "144                      0.0                     0.0   \n",
       "145                      0.0                     0.0   \n",
       "146                      0.0                     0.0   \n",
       "147                      0.0                     0.0   \n",
       "148                      0.0                     0.0   \n",
       "149                      0.0                     0.0   \n",
       "150                      0.0                     0.0   \n",
       "151                      0.0                     0.0   \n",
       "155                      0.0                     0.0   \n",
       "156                      0.0                     0.0   \n",
       "157                      0.0                     0.0   \n",
       "159                      0.0                     0.0   \n",
       "160                      0.0                     0.0   \n",
       "161                      0.0                     0.0   \n",
       "163                      0.0                     0.0   \n",
       "164                      0.0                     0.0   \n",
       "166                      0.0                     0.0   \n",
       "\n",
       "     targetMarket_label_not suitable people vulnerable gluten milk deretives  \\\n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "34                                                 0.0                         \n",
       "35                                                 0.0                         \n",
       "..                                                 ...                         \n",
       "128                                                0.0                         \n",
       "129                                                0.0                         \n",
       "130                                                0.0                         \n",
       "131                                                0.0                         \n",
       "134                                                0.0                         \n",
       "135                                                0.0                         \n",
       "136                                                0.0                         \n",
       "137                                                0.0                         \n",
       "138                                                0.0                         \n",
       "140                                                0.0                         \n",
       "141                                                0.0                         \n",
       "142                                                0.0                         \n",
       "143                                                0.0                         \n",
       "144                                                0.0                         \n",
       "145                                                0.0                         \n",
       "146                                                0.0                         \n",
       "147                                                0.0                         \n",
       "148                                                0.0                         \n",
       "149                                                0.0                         \n",
       "150                                                0.0                         \n",
       "151                                                0.0                         \n",
       "155                                                0.0                         \n",
       "156                                                0.0                         \n",
       "157                                                0.0                         \n",
       "159                                                0.0                         \n",
       "160                                                0.0                         \n",
       "161                                                0.0                         \n",
       "163                                                0.0                         \n",
       "164                                                0.0                         \n",
       "166                                                0.0                         \n",
       "\n",
       "     PCA_allergens_1  PCA_allergens_2  PCA_allergens_M_1  PCA_allergens_M_2  \\\n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "34               1.0        -1.622201          -3.005318          -0.075487   \n",
       "35               1.0        -0.893882          -1.028835           0.435130   \n",
       "..               ...              ...                ...                ...   \n",
       "128              1.0        -0.699354          -0.695291           0.361128   \n",
       "129              1.0        -0.904681          -0.630370          -0.082874   \n",
       "130              1.0        -0.918554          -0.926693           0.373695   \n",
       "131              1.0        -0.597774          -0.643616          -0.202637   \n",
       "134              1.0        -0.597774          -0.643616          -0.202637   \n",
       "135              1.0        -0.597774          -0.643616          -0.202637   \n",
       "136              1.0        -0.597774          -0.643616          -0.202637   \n",
       "137              1.0        -0.597774          -0.643616          -0.202637   \n",
       "138              1.0        -0.597774          -0.643616          -0.202637   \n",
       "140              1.0        -0.597774          -0.643616          -0.202637   \n",
       "141              1.0        -0.597774          -0.643616          -0.202637   \n",
       "142              1.0        -0.597774          -0.643616          -0.202637   \n",
       "143              1.0        -0.597774          -0.643616          -0.202637   \n",
       "144              1.0        -0.597774          -0.643616          -0.202637   \n",
       "145              1.0        -0.597774          -0.643616          -0.202637   \n",
       "146              1.0        -0.597774          -0.643616          -0.202637   \n",
       "147              1.0        -0.597774          -0.643616          -0.202637   \n",
       "148              1.0        -0.597774          -0.643616          -0.202637   \n",
       "149              1.0        -0.597774          -0.643616          -0.202637   \n",
       "150              1.0        -0.597774          -0.643616          -0.202637   \n",
       "151              1.0        -0.597774          -0.643616          -0.202637   \n",
       "155              1.0        -0.597774          -0.643616          -0.202637   \n",
       "156              1.0        -0.597774          -0.643616          -0.202637   \n",
       "157              1.0        -0.597774          -0.643616          -0.202637   \n",
       "159              1.0        -0.597774          -0.643616          -0.202637   \n",
       "160              1.0        -0.597774          -0.643616          -0.202637   \n",
       "161              1.0        -0.597774          -0.643616          -0.202637   \n",
       "163              1.0        -0.597774          -0.643616          -0.202637   \n",
       "164              1.0        -0.597774          -0.643616          -0.202637   \n",
       "166              1.0        -0.597774          -0.643616          -0.202637   \n",
       "\n",
       "       Target  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "34   0.886463  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "35  -0.306756  \n",
       "34   0.886463  \n",
       "34   0.886463  \n",
       "35  -0.306756  \n",
       "..        ...  \n",
       "128 -0.099710  \n",
       "129  0.079337  \n",
       "130 -0.106418  \n",
       "131  0.079399  \n",
       "134  0.079399  \n",
       "135  0.079399  \n",
       "136  0.079399  \n",
       "137  0.079399  \n",
       "138  0.079399  \n",
       "140  0.079399  \n",
       "141  0.079399  \n",
       "142  0.079399  \n",
       "143  0.079399  \n",
       "144  0.079399  \n",
       "145  0.079399  \n",
       "146  0.079399  \n",
       "147  0.079399  \n",
       "148  0.079399  \n",
       "149  0.079399  \n",
       "150  0.079399  \n",
       "151  0.079399  \n",
       "155  0.079399  \n",
       "156  0.079399  \n",
       "157  0.079399  \n",
       "159  0.079399  \n",
       "160  0.079399  \n",
       "161  0.079399  \n",
       "163  0.079399  \n",
       "164  0.079399  \n",
       "166  0.079399  \n",
       "\n",
       "[262 rows x 59 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Machine Learning Models from a List (using the reusable method defined above). Store the results (Accuracy score - train, accuracy score - test, and training time) in a List for data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:Pipeline\n",
      "train time: 984361.000s\n",
      "f1_score:   0.954\n",
      "f1_score_train:   0.922\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.94      0.95        33\n",
      "\n",
      "avg / total       0.94      0.91      0.93        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 2 31]]\n",
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:RidgeClassifier\n",
      "train time: 0.000s\n",
      "f1_score:   0.954\n",
      "f1_score_train:   0.922\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.94      0.95        33\n",
      "\n",
      "avg / total       0.94      0.91      0.93        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 2 31]]\n",
      "dimensionality: 59\n",
      "density: 0.796610\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:Pipeline\n",
      "train time: 0.000s\n",
      "f1_score:   0.937\n",
      "f1_score_train:   0.944\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.91      0.94        33\n",
      "\n",
      "avg / total       0.94      0.88      0.91        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 3 30]]\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:Perceptron\n",
      "train time: 0.000s\n",
      "f1_score:   0.937\n",
      "f1_score_train:   0.944\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.91      0.94        33\n",
      "\n",
      "avg / total       0.94      0.88      0.91        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 3 30]]\n",
      "dimensionality: 59\n",
      "density: 0.644068\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:Pipeline\n",
      "train time: 984344.000s\n",
      "f1_score:   0.954\n",
      "f1_score_train:   0.803\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.94      0.95        33\n",
      "\n",
      "avg / total       0.94      0.91      0.93        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 2 31]]\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:PassiveAggressiveClassifier\n",
      "train time: 0.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.969\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "dimensionality: 59\n",
      "density: 0.728814\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:Pipeline\n",
      "train time: 984371.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.988\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:KNeighborsClassifier\n",
      "train time: 0.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.988\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:Pipeline\n",
      "train time: 671870.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   1.000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:RandomForestClassifier\n",
      "train time: 671874.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   1.000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:LinearSVC\n",
      "train time: 984445.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.952\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "dimensionality: 59\n",
      "density: 0.830508\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:SGDClassifier\n",
      "train time: 0.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.973\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "dimensionality: 59\n",
      "density: 0.593220\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:LinearSVC\n",
      "train time: 984376.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.969\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "dimensionality: 59\n",
      "density: 0.305085\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:SGDClassifier\n",
      "train time: 984344.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.964\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "dimensionality: 59\n",
      "density: 0.627119\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:SGDClassifier\n",
      "train time: 984376.000s\n",
      "f1_score:   0.970\n",
      "f1_score_train:   0.977\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.97      0.97        33\n",
      "\n",
      "avg / total       0.94      0.94      0.94        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 1 32]]\n",
      "dimensionality: 59\n",
      "density: 0.542373\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:NearestCentroid\n",
      "train time: 0.000s\n",
      "f1_score:   0.921\n",
      "f1_score_train:   0.899\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.88      0.92        33\n",
      "\n",
      "avg / total       0.94      0.85      0.89        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 4 29]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "results = []\n",
    "model_name = []\n",
    "\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=5), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    selector_clf = benchmark(Pipeline([('selector', selector),('classifier', clf)]))\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "    model_name.append(name)\n",
    "    #model.append(clf)\n",
    "    \n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,dual=False, tol=1e-3)))\n",
    "    model_name.append(\"LinearSVC\"+\" \"+penalty)\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,penalty=penalty)))\n",
    "    model_name.append(\"SGDClassifier\"+\" \"+penalty)\n",
    "\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,penalty=\"elasticnet\")))\n",
    "model_name.append(\"SGD with Elastic Net penalty\")\n",
    "\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "model_name.append(\"NearestCentroid (aka Rocchio classifier)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 10:26:54.045632  4804 deprecation.py:506] From C:\\Users\\09263248\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0723 10:26:54.280035  4804 deprecation.py:323] From C:\\Users\\09263248\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout (Dropout)            (None, 59)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                3840      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,905\n",
      "Trainable params: 3,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-3\n",
    "epochs=100\n",
    "batch_size=128\n",
    "layers=2\n",
    "units=64\n",
    "dropout_rate=0.2\n",
    "model = mlp_model(layers=layers,units=units,dropout_rate=dropout_rate,input_shape=X_train.shape[1:])\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model and store results in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Keras Dense Neural Network\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "262\n",
      "model name:<tensorflow.python.keras.engine.sequential.Sequential object at 0x0000017F9779BDA0>\n",
      "Train on 262 samples, validate on 34 samples\n",
      "Epoch 1/100\n",
      "262/262 - 0s - loss: 0.7297 - acc: 0.5458 - val_loss: 1.0699 - val_acc: 0.4118\n",
      "Epoch 2/100\n",
      "262/262 - 0s - loss: 0.6917 - acc: 0.5763 - val_loss: 0.9796 - val_acc: 0.4118\n",
      "Epoch 3/100\n",
      "262/262 - 0s - loss: 0.6894 - acc: 0.5916 - val_loss: 0.9148 - val_acc: 0.4706\n",
      "Epoch 4/100\n",
      "262/262 - 0s - loss: 0.6538 - acc: 0.6870 - val_loss: 0.8642 - val_acc: 0.4706\n",
      "Epoch 5/100\n",
      "262/262 - 0s - loss: 0.6219 - acc: 0.6679 - val_loss: 0.8327 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "262/262 - 0s - loss: 0.6068 - acc: 0.7252 - val_loss: 0.7959 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "262/262 - 0s - loss: 0.5699 - acc: 0.7672 - val_loss: 0.7597 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "262/262 - 0s - loss: 0.5532 - acc: 0.7824 - val_loss: 0.7303 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "262/262 - 0s - loss: 0.5509 - acc: 0.7595 - val_loss: 0.7009 - val_acc: 0.7353\n",
      "Epoch 10/100\n",
      "262/262 - 0s - loss: 0.5472 - acc: 0.7977 - val_loss: 0.6855 - val_acc: 0.7353\n",
      "Epoch 11/100\n",
      "262/262 - 0s - loss: 0.5300 - acc: 0.8130 - val_loss: 0.6734 - val_acc: 0.7353\n",
      "Epoch 12/100\n",
      "262/262 - 0s - loss: 0.5132 - acc: 0.8435 - val_loss: 0.6497 - val_acc: 0.7353\n",
      "Epoch 13/100\n",
      "262/262 - 0s - loss: 0.4980 - acc: 0.8435 - val_loss: 0.6162 - val_acc: 0.7353\n",
      "Epoch 14/100\n",
      "262/262 - 0s - loss: 0.4859 - acc: 0.8702 - val_loss: 0.5839 - val_acc: 0.7941\n",
      "Epoch 15/100\n",
      "262/262 - 0s - loss: 0.5017 - acc: 0.8435 - val_loss: 0.5631 - val_acc: 0.7941\n",
      "Epoch 16/100\n",
      "262/262 - 0s - loss: 0.4628 - acc: 0.8855 - val_loss: 0.5456 - val_acc: 0.7941\n",
      "Epoch 17/100\n",
      "262/262 - 0s - loss: 0.4649 - acc: 0.8550 - val_loss: 0.5324 - val_acc: 0.7941\n",
      "Epoch 18/100\n",
      "262/262 - 0s - loss: 0.4376 - acc: 0.8664 - val_loss: 0.5291 - val_acc: 0.8529\n",
      "Epoch 19/100\n",
      "262/262 - 0s - loss: 0.4296 - acc: 0.8626 - val_loss: 0.5339 - val_acc: 0.8529\n",
      "Epoch 20/100\n",
      "262/262 - 0s - loss: 0.4137 - acc: 0.8817 - val_loss: 0.5374 - val_acc: 0.8529\n",
      "Epoch 21/100\n",
      "262/262 - 0s - loss: 0.4282 - acc: 0.8740 - val_loss: 0.5291 - val_acc: 0.8529\n",
      "train time: 388933.000s\n",
      "f1_score:   0.921\n",
      "f1_score_train:   0.939\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         1\n",
      "          1       0.97      0.88      0.92        33\n",
      "\n",
      "avg / total       0.94      0.85      0.89        34\n",
      "\n",
      "confusion matrix:\n",
      "[[ 0  1]\n",
      " [ 4 29]]\n"
     ]
    }
   ],
   "source": [
    "print('=' * 80)\n",
    "print(\"Keras Dense Neural Network\")\n",
    "results.append(benchmark(model))\n",
    "model_name.append(\"Keras Dense Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results from multiple Machine Learning Models (accuracy - train, accuracy - test, training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAI1CAYAAAB8GvSWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGJckSZI6MYxLkiRJnRjGJUmSpE7+Pz0uFKBCzjRzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17f9867e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, f1_score_train, f1_score_test, training_time = results\n",
    "\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, f1_score_train, .2, label=\"f1_score_train\", color='r')\n",
    "plt.barh(indices+0.3, f1_score_test, .2, label=\"f1_score_test\", color='b')\n",
    "plt.barh(indices + .6, training_time, .2, label=\"training time\", color='g')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, model_name):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above results show the Precision and Accuracy of the 'No' cases is zero. On further inspection we find, the Yes/No is governed by a simple business rule, and most of the 'No' entries are data entry errors. There is only genuine 'No' entry in the file 'Lays BIO - FSHA - In Process 04.03.19 (002)'. We recommend a Business Rules based approach for this automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

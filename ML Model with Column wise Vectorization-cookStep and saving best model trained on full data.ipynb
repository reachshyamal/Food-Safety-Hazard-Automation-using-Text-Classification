{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reads text data from data extract created from FSHA Forms and runs predictive Models to predict the value 'Is there any cookStep by the consumer?' , based on the Input Data\n",
    "# It does vectorization of each Column and concatenates these Vectors to create a final Feature Vector and fits the best ML Model found from benchmarking study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all necessary modules\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from scipy import signal\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File name and other important parameters like ngram_range set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_range_inp=(1,2)\n",
    "#File extract with all Yes values for the crossContactAllergens choice\n",
    "filename = \"C:/Pepsico/Base LCS Files Extract_28 Aug 2019_207Files.xlsm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#File extract with all No values for the crossContactAllergens choice\n",
    "filename1 = \"C:/Pepsico/Cook step_21 Files_Extracts 03 Sep 2019.xlsm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define reusable modular method for Text Normalization (removal of stopwords, changing to lower case, removal of punctuation etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data extract file (tabular format with Input data(X) and target(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsha_data = pd.read_excel(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fsha_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Take only Yes choices from first file (there are data entry errors for No choices)\n",
    "fsha_data = fsha_data[(fsha_data['cookstepByConsumer']=='No')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fsha_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsha_data_yes = pd.read_excel(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fsha_data_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenate Yes and No values\n",
    "fsha_data = pd.concat([fsha_data,fsha_data_yes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fsha_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     204\n",
       "Yes     21\n",
       "Name: cookstepByConsumer, dtype: int64"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsha_data.cookstepByConsumer.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>projName</th>\n",
       "      <th>accolNumber</th>\n",
       "      <th>PDA_projName</th>\n",
       "      <th>projType</th>\n",
       "      <th>projDesc</th>\n",
       "      <th>formulaNumber</th>\n",
       "      <th>owner</th>\n",
       "      <th>sector</th>\n",
       "      <th>center</th>\n",
       "      <th>...</th>\n",
       "      <th>prodModifications</th>\n",
       "      <th>newIngredient</th>\n",
       "      <th>approvedPackage</th>\n",
       "      <th>potentialMicrobial</th>\n",
       "      <th>crossContactAllergens</th>\n",
       "      <th>chokeHazard</th>\n",
       "      <th>operationalAllergen</th>\n",
       "      <th>abuseByConsumer</th>\n",
       "      <th>cookstepByConsumer</th>\n",
       "      <th>allergensLabeledIMAF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FSHA 46694 Lays Ridge.xlsm</td>\n",
       "      <td>Lays Ridge launch</td>\n",
       "      <td>46694</td>\n",
       "      <td>Lay's Wavy Spring Onion, Lay's Wavy Sour Cream,</td>\n",
       "      <td>Category Reframe</td>\n",
       "      <td>Significant launch of new subline within exist...</td>\n",
       "      <td>P02806, PP03084 (PF03211)</td>\n",
       "      <td>Artur Zyśk, +48723990114</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Warsaw, Poland</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Milk, Lactose : Milk, Lactose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FSHA 5.4.1 - Doritos Sweet Chilli Pepper Na Re...</td>\n",
       "      <td>Sodium reduction Doritos Sweet Chilli Pepper I...</td>\n",
       "      <td>39660</td>\n",
       "      <td>Doritos Sweet Chilli Pepper</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Sodium reduction project to conform to Legisla...</td>\n",
       "      <td>999010432 Doritos Sweet Chilli Pepper</td>\n",
       "      <td>Sherwin Tlhoaele / Gabisile Buthelezi</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable : Not Applicable : Soya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FSHA 5.4.1 - Doritos Sweet Chilli Pepper Na Re...</td>\n",
       "      <td>Sodium reduction Doritos Sweet Chilli Pepper P...</td>\n",
       "      <td>39660</td>\n",
       "      <td>Doritos Sweet Chilli Pepper</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Sodium reduction project to conform to Legisla...</td>\n",
       "      <td>999010432 Doritos Sweet Chilli Pepper</td>\n",
       "      <td>Sherwin Tlhoaele / Gabisile Buthelezi</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable : Not Applicable : Soya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSHA 5.4.1 - Lay's BBQ Gate 3 Prospecton 5.04....</td>\n",
       "      <td>Lay's Barbecue Flavoured Potato Chips</td>\n",
       "      <td>50169</td>\n",
       "      <td>Lay's Barbecue Flavoured Potato Chips</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>New flavour for the Lay's range - Lay's Barbecue</td>\n",
       "      <td>not provided</td>\n",
       "      <td>Xolelwa Nzuzo</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable : Not Applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FSHA 5.4.1 - Simba Cheese &amp; Onion NA Reduction...</td>\n",
       "      <td>Simba Cheese &amp; Onion Sodium Reduction Isando</td>\n",
       "      <td>39660</td>\n",
       "      <td>Simba Cheese &amp; Onion</td>\n",
       "      <td>Renovation</td>\n",
       "      <td>Sodium Reduction Project to conform to Legisla...</td>\n",
       "      <td>Simba Potato Chips (ZBR for different flavoure...</td>\n",
       "      <td>Lizel Laubscher</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Not Applicable : Not Applicable : Dairy (Cows ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0                         FSHA 46694 Lays Ridge.xlsm   \n",
       "1  FSHA 5.4.1 - Doritos Sweet Chilli Pepper Na Re...   \n",
       "2  FSHA 5.4.1 - Doritos Sweet Chilli Pepper Na Re...   \n",
       "3  FSHA 5.4.1 - Lay's BBQ Gate 3 Prospecton 5.04....   \n",
       "4  FSHA 5.4.1 - Simba Cheese & Onion NA Reduction...   \n",
       "\n",
       "                                            projName accolNumber  \\\n",
       "0                                  Lays Ridge launch       46694   \n",
       "1  Sodium reduction Doritos Sweet Chilli Pepper I...       39660   \n",
       "2  Sodium reduction Doritos Sweet Chilli Pepper P...       39660   \n",
       "3              Lay's Barbecue Flavoured Potato Chips       50169   \n",
       "4       Simba Cheese & Onion Sodium Reduction Isando       39660   \n",
       "\n",
       "                                       PDA_projName          projType  \\\n",
       "0  Lay's Wavy Spring Onion, Lay's Wavy Sour Cream,   Category Reframe   \n",
       "1                       Doritos Sweet Chilli Pepper          Refresh    \n",
       "2                       Doritos Sweet Chilli Pepper          Refresh    \n",
       "3             Lay's Barbecue Flavoured Potato Chips           Refresh   \n",
       "4                              Simba Cheese & Onion        Renovation   \n",
       "\n",
       "                                            projDesc  \\\n",
       "0  Significant launch of new subline within exist...   \n",
       "1  Sodium reduction project to conform to Legisla...   \n",
       "2  Sodium reduction project to conform to Legisla...   \n",
       "3   New flavour for the Lay's range - Lay's Barbecue   \n",
       "4  Sodium Reduction Project to conform to Legisla...   \n",
       "\n",
       "                                       formulaNumber  \\\n",
       "0                         P02806, PP03084 (PF03211)    \n",
       "1              999010432 Doritos Sweet Chilli Pepper   \n",
       "2              999010432 Doritos Sweet Chilli Pepper   \n",
       "3                                       not provided   \n",
       "4  Simba Potato Chips (ZBR for different flavoure...   \n",
       "\n",
       "                                    owner sector          center  \\\n",
       "0                Artur Zyśk, +48723990114   ESSA  Warsaw, Poland   \n",
       "1  Sherwin Tlhoaele / Gabisile Buthelezi    ESSA    South Africa   \n",
       "2  Sherwin Tlhoaele / Gabisile Buthelezi    ESSA    South Africa   \n",
       "3                           Xolelwa Nzuzo   ESSA    South Africa   \n",
       "4                         Lizel Laubscher   ESSA    South Africa   \n",
       "\n",
       "                         ...                         prodModifications  \\\n",
       "0                        ...                                        No   \n",
       "1                        ...                                        No   \n",
       "2                        ...                                        No   \n",
       "3                        ...                                        No   \n",
       "4                        ...                                        No   \n",
       "\n",
       "  newIngredient approvedPackage potentialMicrobial crossContactAllergens  \\\n",
       "0           Yes              No                 No                   Yes   \n",
       "1            No              No                 No                    No   \n",
       "2            No              No                 No                    No   \n",
       "3           Yes              No                 No                   Yes   \n",
       "4            No              No                 No                   Yes   \n",
       "\n",
       "  chokeHazard operationalAllergen abuseByConsumer cookstepByConsumer  \\\n",
       "0          No                 Yes              No                 No   \n",
       "1          No                 Yes              No                 No   \n",
       "2          No                 Yes              No                 No   \n",
       "3          No                 Yes              No                 No   \n",
       "4          No                 Yes              No                 No   \n",
       "\n",
       "                                allergensLabeledIMAF  \n",
       "0                      Milk, Lactose : Milk, Lactose  \n",
       "1             Not Applicable : Not Applicable : Soya  \n",
       "2             Not Applicable : Not Applicable : Soya  \n",
       "3                    Not Applicable : Not Applicable  \n",
       "4  Not Applicable : Not Applicable : Dairy (Cows ...  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 703,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsha_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Analysis select the Features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selecting set of columns as Features\n",
    "features_df=fsha_data[['projDesc','CPD-ProdName','cookedOrHeated','labelingInstructions','CPD-ProdName-Desc','packMaterial']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace missing values in features with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define reusable code to Vectorize Text column (ex:labelingInstructions) using TF-IDF Vectorizer, after doing Text data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of text data using TF-IDF Vectorizer\n",
    "\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "n_gram_range = (1,2)\n",
    "ngram_range = n_gram_range\n",
    "kwargs = {\n",
    "            'ngram_range': ngram_range,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels,ngram_range):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    \n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize each column , by cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['projDesc'] = normalize_corpus(train_df['projDesc'])\n",
    "train_df['CPD-ProdName-Desc']=normalize_corpus(train_df['CPD-ProdName-Desc'])\n",
    "train_df['CPD-ProdName']=normalize_corpus(train_df['CPD-ProdName'])\n",
    "train_df['cookedOrHeated']=normalize_corpus(train_df['cookedOrHeated'])\n",
    "train_df['labelingInstructions']=normalize_corpus(train_df['labelingInstructions'])\n",
    "train_df['packMaterial']=normalize_corpus(train_df['packMaterial'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['significant launch new subline within existing pc brand ridged wavy lays bal q3 2018 launch regular flavors onion fromage ridged pc base permanently produced grodzisk plant'\n",
      " 'sodium reduction project conform legislative requirements south africa no new hazard allergens introduced plant seasoning not require process modified packaging material pack format remain per current'\n",
      " 'sodium reduction project conform legislative requirements south africa no new hazard allergens introduced plant seasoning not require process modified packaging material pack format remain per current'\n",
      " 'new flavour lays range lays barbecue'\n",
      " 'sodium reduction project conform legislative requirements south africa no new hazard allergens introduced plant seasoning not require process modified packaging material pack format remain per current']\n"
     ]
    }
   ],
   "source": [
    "#Check the first five values\n",
    "print(train_df['projDesc'][:5].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize the target (1/0 for Yes/No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics \n",
    "\n",
    "def impute_target(fsha_data,targetName):\n",
    "    train_y=[]\n",
    "    for i in range (len(fsha_data)):\n",
    "        if fsha_data[targetName].values[i]=='Yes':\n",
    "            train_y.append(1)\n",
    "        elif fsha_data[targetName].values[i]=='No':\n",
    "            train_y.append(0)\n",
    "        else:\n",
    "            train_y.append(-1)\n",
    "               \n",
    "    mode_y = statistics.mode(train_y)\n",
    "\n",
    "    for i in range (len(fsha_data)):\n",
    "        if train_y[i]==-1:\n",
    "            train_y[i] = mode_y\n",
    "            \n",
    "    return train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target (Yes/No choice) in PDAF are converted to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_cookstepByConsumer = impute_target(fsha_data,\"cookstepByConsumer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['target']=train_y_cookstepByConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    204\n",
       "1     21\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df['target']\n",
    "train_df = train_df.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform n-gram vectorization and PCA on text data, columnwise (6 components per column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean = False,with_std = False)\n",
    "\n",
    "n_components = 6\n",
    "#whiten = False\n",
    "#random_state = 42\n",
    "svd_solver=\"full\"\n",
    "svd_solver = \"randomized\"\n",
    "#pca = PCA(n_components=n_components,svd_solver=svd_solver,whiten=whiten, random_state=42)\n",
    "\n",
    "\n",
    "def preprocess_text(train_df,y):    \n",
    "    train_labels = y\n",
    "    x_ngram_projDesc = ngram_vectorize(train_df['projDesc'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_CPD_ProdName_Desc = ngram_vectorize(train_df['CPD-ProdName-Desc'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_CPD_ProdName = ngram_vectorize(train_df['CPD-ProdName'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_cookedOrHeated = ngram_vectorize(train_df['cookedOrHeated'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_labelingInstructions = ngram_vectorize(train_df['labelingInstructions'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_packMaterial = ngram_vectorize(train_df['packMaterial'], train_labels,n_gram_range).toarray()\n",
    "   \n",
    "    \n",
    "\n",
    "    x_ngram_projDesc = scaler.fit_transform(x_ngram_projDesc)\n",
    "    x_ngram_packMaterial = scaler.fit_transform(x_ngram_packMaterial)\n",
    "    x_ngram_CPD_ProdName = scaler.fit_transform(x_ngram_CPD_ProdName)\n",
    "    x_ngram_CPD_ProdName_Desc = scaler.fit_transform(x_ngram_CPD_ProdName_Desc)\n",
    "    x_ngram_labelingInstructions = scaler.fit_transform(x_ngram_labelingInstructions)\n",
    "    x_ngram_cookedOrHeated = scaler.fit_transform(x_ngram_cookedOrHeated)\n",
    "    \n",
    "    x_pca_projDesc = pca.fit_transform(x_ngram_projDesc)\n",
    "    x_pca_packMaterial = pca.fit_transform(x_ngram_packMaterial)\n",
    "    x_pca_CPD_ProdName = pca.fit_transform(x_ngram_CPD_ProdName)\n",
    "    x_pca_CPD_ProdName_Desc = pca.fit_transform(x_ngram_CPD_ProdName_Desc)\n",
    "    x_pca_labelingInstructions = pca.fit_transform(x_ngram_labelingInstructions)\n",
    "    x_pca_cookedOrHeated = pca.fit_transform(x_ngram_cookedOrHeated)\n",
    "    x_train = np.concatenate((x_pca_projDesc, x_pca_packMaterial,x_pca_CPD_ProdName,x_pca_CPD_ProdName_Desc,x_pca_labelingInstructions,x_pca_cookedOrHeated),axis=1)\n",
    "    print(x_train.shape)\n",
    "    return x_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 36)\n",
      "StandardScaler(copy=True, with_mean=False, with_std=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_features = preprocess_text(train_df,y)\n",
    "joblib.dump(scaler, \"scaler_cookstep.pkl\")\n",
    "joblib.dump(pca, \"pca_cookstep.pkl\")\n",
    "joblib.dump(vectorizer,\"vectorizer_cookstep.pkl\")\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler_load = joblib.load(\"scaler_cookstep.pkl\")\n",
    "pca_load = joblib.load(\"pca_cookstep.pkl\")\n",
    "vectorizer_load = joblib.load(\"vectorizer_cookstep.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y, test_size=.2, random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 36)\n",
      "(45, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    163\n",
       "1     17\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    41\n",
       "1     4\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180,)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.202181</td>\n",
       "      <td>-0.069112</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>-0.033781</td>\n",
       "      <td>-0.121655</td>\n",
       "      <td>-0.008141</td>\n",
       "      <td>0.375590</td>\n",
       "      <td>-0.080343</td>\n",
       "      <td>0.394614</td>\n",
       "      <td>-0.136807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.663360</td>\n",
       "      <td>-0.066175</td>\n",
       "      <td>-0.096046</td>\n",
       "      <td>0.295722</td>\n",
       "      <td>0.568651</td>\n",
       "      <td>0.873505</td>\n",
       "      <td>-0.080540</td>\n",
       "      <td>-0.033097</td>\n",
       "      <td>-0.036639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.144061</td>\n",
       "      <td>-0.083741</td>\n",
       "      <td>-0.025922</td>\n",
       "      <td>-0.014495</td>\n",
       "      <td>-0.140741</td>\n",
       "      <td>-0.026255</td>\n",
       "      <td>0.622526</td>\n",
       "      <td>0.702727</td>\n",
       "      <td>-0.273998</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621406</td>\n",
       "      <td>0.110367</td>\n",
       "      <td>-0.022483</td>\n",
       "      <td>-0.053939</td>\n",
       "      <td>-0.576162</td>\n",
       "      <td>-0.090327</td>\n",
       "      <td>-0.019413</td>\n",
       "      <td>-0.008665</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>-0.006527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.137128</td>\n",
       "      <td>-0.071987</td>\n",
       "      <td>-0.023578</td>\n",
       "      <td>-0.068925</td>\n",
       "      <td>-0.073829</td>\n",
       "      <td>0.114494</td>\n",
       "      <td>-0.596131</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>-0.047097</td>\n",
       "      <td>-0.013474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.663360</td>\n",
       "      <td>-0.066175</td>\n",
       "      <td>-0.096046</td>\n",
       "      <td>0.295722</td>\n",
       "      <td>0.568651</td>\n",
       "      <td>0.873505</td>\n",
       "      <td>-0.080540</td>\n",
       "      <td>-0.033097</td>\n",
       "      <td>-0.036639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.278907</td>\n",
       "      <td>-0.080425</td>\n",
       "      <td>-0.416639</td>\n",
       "      <td>0.700884</td>\n",
       "      <td>0.375808</td>\n",
       "      <td>0.118172</td>\n",
       "      <td>-0.596131</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>-0.047097</td>\n",
       "      <td>-0.013474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071302</td>\n",
       "      <td>0.026018</td>\n",
       "      <td>-0.006303</td>\n",
       "      <td>-0.019942</td>\n",
       "      <td>-0.576162</td>\n",
       "      <td>-0.090327</td>\n",
       "      <td>-0.019413</td>\n",
       "      <td>-0.008665</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>-0.006527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.164305</td>\n",
       "      <td>-0.060840</td>\n",
       "      <td>-0.039161</td>\n",
       "      <td>-0.022498</td>\n",
       "      <td>-0.104583</td>\n",
       "      <td>-0.032702</td>\n",
       "      <td>-0.596131</td>\n",
       "      <td>0.022430</td>\n",
       "      <td>-0.047097</td>\n",
       "      <td>-0.013474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.663360</td>\n",
       "      <td>-0.066175</td>\n",
       "      <td>-0.096046</td>\n",
       "      <td>0.295722</td>\n",
       "      <td>0.568651</td>\n",
       "      <td>0.873505</td>\n",
       "      <td>-0.080540</td>\n",
       "      <td>-0.033097</td>\n",
       "      <td>-0.036639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.202181 -0.069112 -0.056826 -0.033781 -0.121655 -0.008141  0.375590   \n",
       "1 -0.144061 -0.083741 -0.025922 -0.014495 -0.140741 -0.026255  0.622526   \n",
       "2 -0.137128 -0.071987 -0.023578 -0.068925 -0.073829  0.114494 -0.596131   \n",
       "3 -0.278907 -0.080425 -0.416639  0.700884  0.375808  0.118172 -0.596131   \n",
       "4 -0.164305 -0.060840 -0.039161 -0.022498 -0.104583 -0.032702 -0.596131   \n",
       "\n",
       "         7         8         9     ...           26        27        28  \\\n",
       "0 -0.080343  0.394614 -0.136807    ...     0.631579  0.663360 -0.066175   \n",
       "1  0.702727 -0.273998 -0.007521    ...    -0.621406  0.110367 -0.022483   \n",
       "2  0.022430 -0.047097 -0.013474    ...     0.631579  0.663360 -0.066175   \n",
       "3  0.022430 -0.047097 -0.013474    ...    -0.071302  0.026018 -0.006303   \n",
       "4  0.022430 -0.047097 -0.013474    ...     0.631579  0.663360 -0.066175   \n",
       "\n",
       "         29        30        31        32        33        34        35  \n",
       "0 -0.096046  0.295722  0.568651  0.873505 -0.080540 -0.033097 -0.036639  \n",
       "1 -0.053939 -0.576162 -0.090327 -0.019413 -0.008665  0.009567 -0.006527  \n",
       "2 -0.096046  0.295722  0.568651  0.873505 -0.080540 -0.033097 -0.036639  \n",
       "3 -0.019942 -0.576162 -0.090327 -0.019413 -0.008665  0.009567 -0.006527  \n",
       "4 -0.096046  0.295722  0.568651  0.873505 -0.080540 -0.033097 -0.036639  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample the cookstepByConsumer = 'Yes' data, since the data is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "not_cookstepByConsumer = X[X.target==0]\n",
    "cookstepByConsumer = X[X.target==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_cookstepByConsumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cookstepByConsumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "cookstepByConsumer_upsampled = resample(cookstepByConsumer,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_cookstepByConsumer), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([cookstepByConsumer_upsampled, not_cookstepByConsumer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    163\n",
       "0    163\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = upsampled.target\n",
    "X_train = upsampled.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, 36)"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the RandomForestClassifier since the benchmarking study shows this model giving best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:   1.000\n",
      "f1_score_train:   1.000\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        41\n",
      "          1       1.00      1.00      1.00         4\n",
      "\n",
      "avg / total       1.00      1.00      1.00        45\n",
      "\n",
      "confusion matrix:\n",
      "[[41  0]\n",
      " [ 0  4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector_clf = Pipeline([('selector', selector),('classifier', RandomForestClassifier(n_estimators=100))])\n",
    "selector_clf.fit(X_train, y_train)\n",
    "pred = selector_clf.predict(X_test)\n",
    "pred_train = selector_clf.predict(X_train)\n",
    "\n",
    "f1_score = metrics.f1_score(y_test, pred)\n",
    "print(\"f1_score:   %0.3f\" % f1_score )\n",
    "    \n",
    "f1_score_train = metrics.f1_score(y_train, pred_train)\n",
    "print(\"f1_score_train:   %0.3f\" % f1_score_train )\n",
    "    \n",
    "print(\"classification report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "    \n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, pred))\n",
    "y_pred = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us plot the Confusion Matrix for cookStepByConsumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATkAAAEYCAYAAAAnEYFiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22e14b0a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "   # else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names=['No','Yes']\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "#                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above results show the Precision and Accuracy of the 'Yes' cases is maximum with RandomForestClassifier , after taking 6 components in PCA, with StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv_results = cross_validate(selector_clf.fit(X_train, y_train), X_train, y_train,cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us do k fold CV on full data and verify the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_features = pd.DataFrame(X_features)\n",
    "y = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_features.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenate our training data back together\n",
    "X = pd.concat([X_features, y], axis=1)\n",
    "\n",
    "# separate minority and majority classes\n",
    "not_cookstepByConsumer = X[X.target==0]\n",
    "cookstepByConsumer = X[X.target==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(not_cookstepByConsumer))\n",
    "print(len(cookstepByConsumer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upsample minority\n",
    "cookstepByConsumer_upsampled = resample(cookstepByConsumer,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_cookstepByConsumer), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([cookstepByConsumer_upsampled, not_cookstepByConsumer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    204\n",
       "0    204\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = upsampled.target\n",
    "X_train = upsampled.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "#We define custom functions to return count of TP,TN,FP,FN for each fold\n",
    "def tn(y_train, pred_train): return confusion_matrix(y_train, pred_train)[0, 0]\n",
    "def fp(y_train, pred_train): return confusion_matrix(y_train, pred_train)[0, 1]\n",
    "def fn(y_train, pred_train): return confusion_matrix(y_train, pred_train)[1, 0]\n",
    "def tp(y_train, pred_train): return confusion_matrix(y_train, pred_train)[1, 1]\n",
    "\n",
    "scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector_clf = Pipeline([('selector', selector),('classifier', RandomForestClassifier(n_estimators=100))])\n",
    "#selector_clf.fit(X_train, y_train)\n",
    "cv_results = cross_validate(selector_clf.fit(X_train, y_train), X_train, y_train,scoring=scoring, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of True Positives\n",
      "[47 43 37 32 38]\n",
      "Count of True Negatives\n",
      "[35 39 45 50 44]\n",
      "Count of False Negatives\n",
      "[0 0 0 0 0]\n",
      "Count of False Positives\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of True Positives\")\n",
    "print(cv_results['test_tp']) \n",
    "print(\"Count of True Negatives\")\n",
    "print(cv_results['test_tn']) \n",
    "# Getting the test set false negative scores\n",
    "print(\"Count of False Negatives\")\n",
    "print(cv_results['test_fn']) \n",
    "print(\"Count of False Positives\")\n",
    "print(cv_results['test_fp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv_results = cross_validate(selector_clf.fit(X_train, y_train), X_train, y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   1.   0.96 1.   1.  ]\n"
     ]
    }
   ],
   "source": [
    "print(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model for predictions on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model_cookstepByConsumer.sav']"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "filename = 'finalized_model_cookstepByConsumer.sav'\n",
    "joblib.dump(selector_clf,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df[:41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'means_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-752-e660de6981ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'means_'"
     ]
    }
   ],
   "source": [
    "means = pca.means_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import fast_dot\n",
    "td = test_data - means\n",
    "tdd = fast_dot(td, pca.components_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_ngram_projDesc = vectorizer_load.transform(train_df['projDesc'])\n",
    "x_ngram_projDesc = scaler_load.transform(x_ngram_projDesc)\n",
    "x_pca_projDesc = pca_load.transform(x_ngram_projDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-750-26ed0e0fc673>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mX_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-750-26ed0e0fc673>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(train_df)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mx_ngram_cookedOrHeated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ngram_cookedOrHeated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mx_pca_projDesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ngram_projDesc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mx_pca_packMaterial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ngram_packMaterial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mx_pca_CPD_ProdName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca_load\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ngram_CPD_ProdName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\decomposition\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mean_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'components_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[1;32m--> 400\u001b[1;33m                                       force_all_finite)\n\u001b[0m\u001b[0;32m    401\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[0;32m    245\u001b[0m                         \u001b[1;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "def preprocess_text(train_df):    \n",
    "    \n",
    "  \n",
    "    x_ngram_projDesc = vectorizer_load.transform(train_df['projDesc'])\n",
    "    x_ngram_CPD_ProdName_Desc = vectorizer_load.transform (train_df['CPD-ProdName-Desc'])\n",
    "    x_ngram_CPD_ProdName = vectorizer_load.transform (train_df['CPD-ProdName'])\n",
    "    x_ngram_cookedOrHeated = vectorizer_load.transform (train_df['cookedOrHeated'])\n",
    "    x_ngram_labelingInstructions = vectorizer_load.transform (train_df['labelingInstructions'])\n",
    "    x_ngram_packMaterial = vectorizer_load.transform (train_df['packMaterial'])\n",
    "    \n",
    "    x_ngram_projDesc = scaler_load.transform(x_ngram_projDesc)\n",
    "    x_ngram_packMaterial = scaler_load.transform(x_ngram_packMaterial)\n",
    "    x_ngram_CPD_ProdName = scaler_load.transform(x_ngram_CPD_ProdName)\n",
    "    x_ngram_CPD_ProdName_Desc = scaler_load.transform(x_ngram_CPD_ProdName_Desc)\n",
    "    x_ngram_labelingInstructions = scaler_load.transform(x_ngram_labelingInstructions)\n",
    "    x_ngram_cookedOrHeated = scaler_load.transform(x_ngram_cookedOrHeated)\n",
    "    \n",
    "    x_pca_projDesc = pca_load.transform(x_ngram_projDesc)\n",
    "    x_pca_packMaterial = pca_load.transform(x_ngram_packMaterial)\n",
    "    x_pca_CPD_ProdName = pca_load.transform(x_ngram_CPD_ProdName)\n",
    "    x_pca_CPD_ProdName_Desc = pca_load.transform(x_ngram_CPD_ProdName_Desc)\n",
    "    x_pca_labelingInstructions = pca_load.transform(x_ngram_labelingInstructions)\n",
    "    x_pca_cookedOrHeated = pca_load.transform(x_ngram_cookedOrHeated)\n",
    "    \n",
    "    x_train = np.concatenate((x_pca_projDesc, x_pca_packMaterial,x_pca_CPD_ProdName,x_pca_CPD_ProdName_Desc,x_pca_labelingInstructions,x_pca_cookedOrHeated),axis=1)\n",
    "   # print(x_train.shape)\n",
    "    return x_train  \n",
    " \n",
    " \n",
    "# In[100]:\n",
    " \n",
    " \n",
    " \n",
    "X_features = preprocess_text(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

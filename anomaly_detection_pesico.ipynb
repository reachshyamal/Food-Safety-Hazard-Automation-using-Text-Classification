{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook reads text data from data extract created from FSHA Forms and runs predictive Models to predict the value 'Are there any inherent or cross contact allergens or intolerants?' , based on the Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necessary modules\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from scipy import signal\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File name and other important parameters like ngram_range set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#These parameters will be input from command line\n",
    "ngram_range_inp=(1,2)\n",
    "filename = \"C:\\\\Pepsico\\\\FSHA RPA - 23 July 2019.xlsm\"\n",
    "#n_components = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define reusable modular method for Text Normalization (removal of stopwords, changing to lower case, removal of punctuation etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\09263250\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "exclude = set(string.punctuation) \n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['from','dtype','object']\n",
    "stop_words.extend(newStopWords)\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # tokenize document\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each word\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # convert to lower case\n",
    "    lower_tokens = [w.lower() for w in tokens]\n",
    "    #remove spaces\n",
    "    stripped = [w.strip() for w in lower_tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in words if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "#normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    #corpus = str(corpus)\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "        # remove special characters    \n",
    "        if special_char_removal:\n",
    "            doc = remove_special_characters(doc)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_doc(corpus):\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    #corpus = str(corpus)\n",
    "    for doc in corpus:\n",
    "\t# split into tokens by white space\n",
    "        doc=str(doc)\n",
    "        doc = doc.replace('\\n',' ')\n",
    "        tokens = doc.split()\n",
    "        # prepare regex for char filtering\n",
    "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        # remove punctuation from each word\n",
    "        tokens = [re_punc.sub('', w) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # remove nn from each word\n",
    "        tokens = [re.sub('nn',' ',word) for word in tokens]\n",
    "        tokens = ' '.join(tokens)\n",
    "        normalized_corpus.append(tokens)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data extract file (tabular format with Input data(X) and target(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsha_data = pd.read_excel(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>projName</th>\n",
       "      <th>accolNumber</th>\n",
       "      <th>PDA_projName</th>\n",
       "      <th>projType</th>\n",
       "      <th>projDesc</th>\n",
       "      <th>formulaNumber</th>\n",
       "      <th>owner</th>\n",
       "      <th>sector</th>\n",
       "      <th>center</th>\n",
       "      <th>...</th>\n",
       "      <th>Table1_Row6_Molluscs</th>\n",
       "      <th>Table1_Row6_Mustard</th>\n",
       "      <th>Table1_Row6_Sesame Seeds</th>\n",
       "      <th>Table1_Row6_Sulphites</th>\n",
       "      <th>Table1_Row1_Moulluscs</th>\n",
       "      <th>Table1_Row2_Moulluscs</th>\n",
       "      <th>Table1_Row3_Moulluscs</th>\n",
       "      <th>Table1_Row4_Moulluscs</th>\n",
       "      <th>Table1_Row5_Moulluscs</th>\n",
       "      <th>Table1_Row6_Moulluscs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#46565 FSHA 5.4.1Star Project G3 v2 + FS input...</td>\n",
       "      <td>S-T3-Star-• POL Star Puff (Chrupki) quality –POL</td>\n",
       "      <td>46565</td>\n",
       "      <td>1SKU Star Puffs Onion\\n2 SKU Star Puffs Cheese...</td>\n",
       "      <td>Brand Refresh</td>\n",
       "      <td>Star Puffs Cheese &amp; STar Hyper Cheese\\nSeasoni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weronika Baranowska</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Warsaw</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#53697 FSHA HT Baguette 4 Cheese UA 2.07.19.xlsm</td>\n",
       "      <td>HT Baguette Four Cheese Flavor</td>\n",
       "      <td>53697</td>\n",
       "      <td>Hrusteam Baguette</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Launch new seasoning 4 Cheese NL-502-352-9 on ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anna Nikonova</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#57686 FSHA 5.4.1 Red Caviar  Azov.xlsm</td>\n",
       "      <td>Lay's Caviar IO 2019 RUS Asov</td>\n",
       "      <td>57686</td>\n",
       "      <td>Lay's Red Caviar</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Idea is to launch I&amp;O flavour under New Year p...</td>\n",
       "      <td>Not provided</td>\n",
       "      <td>Evgeniy Shklovskiy +79163257848</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#57686 FSHA 5.4.1 Red Caviar Kashira.xlsm</td>\n",
       "      <td>Lay's Caviar IO 2019 RUS Kashira</td>\n",
       "      <td>57686</td>\n",
       "      <td>Lay's Red Caviar</td>\n",
       "      <td>Refresh</td>\n",
       "      <td>Idea is to launch I&amp;O flavour under New Year p...</td>\n",
       "      <td>Not provided</td>\n",
       "      <td>Evgeniy Shklovskiy +79163257848</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53354-FSHA-In Process 13.12.18.xlsm</td>\n",
       "      <td>Soft and Mild Iberia 2019</td>\n",
       "      <td>53354</td>\n",
       "      <td>Cheetos Palomito</td>\n",
       "      <td>Re Launch</td>\n",
       "      <td>Re Launch of Cheetos Palomitos, Soft Extruded ...</td>\n",
       "      <td>CP2019</td>\n",
       "      <td>David Labrado 07770646572</td>\n",
       "      <td>ESSA</td>\n",
       "      <td>Beaumont Park</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File Name  \\\n",
       "0  #46565 FSHA 5.4.1Star Project G3 v2 + FS input...   \n",
       "1   #53697 FSHA HT Baguette 4 Cheese UA 2.07.19.xlsm   \n",
       "2            #57686 FSHA 5.4.1 Red Caviar  Azov.xlsm   \n",
       "3          #57686 FSHA 5.4.1 Red Caviar Kashira.xlsm   \n",
       "4                53354-FSHA-In Process 13.12.18.xlsm   \n",
       "\n",
       "                                            projName accolNumber  \\\n",
       "0  S-T3-Star-• POL Star Puff (Chrupki) quality –POL        46565   \n",
       "1                     HT Baguette Four Cheese Flavor       53697   \n",
       "2                      Lay's Caviar IO 2019 RUS Asov       57686   \n",
       "3                   Lay's Caviar IO 2019 RUS Kashira       57686   \n",
       "4                          Soft and Mild Iberia 2019       53354   \n",
       "\n",
       "                                        PDA_projName       projType  \\\n",
       "0  1SKU Star Puffs Onion\\n2 SKU Star Puffs Cheese...  Brand Refresh   \n",
       "1                                  Hrusteam Baguette        Refresh   \n",
       "2                                  Lay's Red Caviar         Refresh   \n",
       "3                                  Lay's Red Caviar         Refresh   \n",
       "4                                   Cheetos Palomito      Re Launch   \n",
       "\n",
       "                                            projDesc formulaNumber  \\\n",
       "0  Star Puffs Cheese & STar Hyper Cheese\\nSeasoni...           NaN   \n",
       "1  Launch new seasoning 4 Cheese NL-502-352-9 on ...           NaN   \n",
       "2  Idea is to launch I&O flavour under New Year p...  Not provided   \n",
       "3  Idea is to launch I&O flavour under New Year p...  Not provided   \n",
       "4  Re Launch of Cheetos Palomitos, Soft Extruded ...        CP2019   \n",
       "\n",
       "                             owner sector         center  \\\n",
       "0              Weronika Baranowska   ESSA         Warsaw   \n",
       "1                    Anna Nikonova   ESSA         Moscow   \n",
       "2  Evgeniy Shklovskiy +79163257848   ESSA         Moscow   \n",
       "3  Evgeniy Shklovskiy +79163257848   ESSA         Moscow   \n",
       "4        David Labrado 07770646572   ESSA  Beaumont Park   \n",
       "\n",
       "           ...          Table1_Row6_Molluscs Table1_Row6_Mustard  \\\n",
       "0          ...                           0.0                   1   \n",
       "1          ...                           0.0                   1   \n",
       "2          ...                           0.0                   0   \n",
       "3          ...                           0.0                   0   \n",
       "4          ...                           0.0                   0   \n",
       "\n",
       "  Table1_Row6_Sesame Seeds Table1_Row6_Sulphites Table1_Row1_Moulluscs  \\\n",
       "0                        0                     0                   NaN   \n",
       "1                        0                     0                   NaN   \n",
       "2                        0                     0                   NaN   \n",
       "3                        0                     0                   NaN   \n",
       "4                        0                     0                   NaN   \n",
       "\n",
       "  Table1_Row2_Moulluscs Table1_Row3_Moulluscs Table1_Row4_Moulluscs  \\\n",
       "0                   NaN                   NaN                   NaN   \n",
       "1                   NaN                   NaN                   NaN   \n",
       "2                   NaN                   NaN                   NaN   \n",
       "3                   NaN                   NaN                   NaN   \n",
       "4                   NaN                   NaN                   NaN   \n",
       "\n",
       "  Table1_Row5_Moulluscs Table1_Row6_Moulluscs  \n",
       "0                   NaN                   NaN  \n",
       "1                   NaN                   NaN  \n",
       "2                   NaN                   NaN  \n",
       "3                   NaN                   NaN  \n",
       "4                   NaN                   NaN  \n",
       "\n",
       "[5 rows x 136 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsha_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on Analysis select the Features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selecting set of columns as Features\n",
    "features_df=fsha_data[['preservatives', 'pH', 'waterActivity', 'packaging','otherFSA',\n",
    "            'prodStorageDist', 'foodSafetyProdClaims','targetMarket','allergens','newIngredient','allergensLabeledIMAF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define reusable code to Vectorize Text column (ex: Allergens) using TF-IDF Vectorizer, after doing Text data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorization of text data using TF-IDF Vectorizer\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "#NGRAM_RANGE \n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "#TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels,ngram_range):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': ngram_range,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    #selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    #selector.fit(x_train, train_labels)\n",
    "    #x_train = selector.transform(x_train)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define method to concatenate normalized Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['preservatives', 'pH', 'waterActivity', 'packaging', 'otherFSA',\n",
       "       'prodStorageDist', 'foodSafetyProdClaims', 'targetMarket', 'allergens',\n",
       "       'newIngredient', 'allergensLabeledIMAF'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[['preservatives', 'pH', 'waterActivity', 'packaging','otherFSA',\n",
    "#            'prodStorageDist', 'foodSafetyProdClaims','targetMarket','allergens','newIngredient']]\n",
    "\n",
    "train_df['norm_preservatives'] = normalize_corpus(train_df['preservatives'])\n",
    "train_df['norm_pH'] = normalize_corpus(train_df['pH'])\n",
    "train_df['norm_waterActivity'] = clean_doc(train_df['waterActivity'])\n",
    "train_df['norm_packaging'] = normalize_corpus(train_df['packaging'])\n",
    "train_df['norm_otherFSA'] = normalize_corpus(train_df['otherFSA'])\n",
    "train_df['norm_prodStorageDist'] = normalize_corpus(train_df['prodStorageDist'])\n",
    "train_df['norm_foodSafety_prodClaims'] = normalize_corpus(train_df['foodSafetyProdClaims'])\n",
    "train_df['norm_targetMarket'] = normalize_corpus(train_df['targetMarket'])\n",
    "train_df['norm_newIngredient'] = normalize_corpus(train_df['newIngredient'])\n",
    "train_df['norm_allergens'] = normalize_corpus(train_df['allergens'])\n",
    "train_df['norm_allergens_M']=normalize_corpus(train_df['allergensLabeledIMAF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['allergens',\n",
    " 'newIngredient',\n",
    " 'pH',\n",
    " 'prodStorageDist',\n",
    " 'waterActivity',\n",
    " 'packaging',\n",
    " 'preservatives',\n",
    " 'otherFSA',\n",
    " 'foodSafetyProdClaims',\n",
    " 'targetMarket','allergensLabeledIMAF'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    yes\n",
      "1    yes\n",
      "2     no\n",
      "3     no\n",
      "4     no\n",
      "Name: norm_newIngredient, dtype: object\n",
      "0    allergens seasonings cq69 vegetable blend mcco...\n",
      "1    no new allergen line seasonings 4 cheese conta...\n",
      "2    inherent milk lactose wheat gluten fish compon...\n",
      "3    inherent inherent milk lactose wheat gluten fi...\n",
      "4                                       milk seasoning\n",
      "Name: norm_allergens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['norm_newIngredient'][:5])\n",
    "print(train_df['norm_allergens'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#selecting set of columns for labelling\n",
    "\n",
    "#ph value label\n",
    "train_df['pH_label']=train_df['norm_pH'].apply(lambda x:re.findall(r'[0-9]+', str(x)) if bool(re.search(r'\\d', str(x))) else str(0))\n",
    "def ph_process(x):\n",
    "    if(len(x)>0):\n",
    "        x = str(x[0])\n",
    "    else:\n",
    "        x = str(x)\n",
    "    return x\n",
    "train_df['pH_label']=train_df['pH_label'].apply(lambda x:ph_process(x))    \n",
    "\n",
    "#'prod_storageDist'column label\n",
    "def prod_storageDist(x):\n",
    "    x=str(x.lower())\n",
    "    if x.__contains__('sun'):\n",
    "            return 'Keep away from sun'\n",
    "    elif x.__contains__('ambient')  :\n",
    "              return 'ambient'\n",
    "    elif x.__contains__('rte'):\n",
    "        return 'RTE'\n",
    "    else:\n",
    "        return 'NA'\n",
    "train_df['prod_storageDist_label']=train_df['norm_prodStorageDist'].apply(lambda x:prod_storageDist(x))                                                              \n",
    "\n",
    "#water activity\n",
    "def water_activity(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if x.__contains__('low'):\n",
    "        return 'low'\n",
    "    elif x.__contains__('max'):\n",
    "        return 'max'\n",
    "    elif x==str(np.nan):\n",
    "        return 'NA'\n",
    "    elif bool(re.search('none|n/a|na',x)):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return(x)\n",
    "train_df['waterActivity_label']=train_df['norm_waterActivity'].apply(lambda x:water_activity(x))                                                              \n",
    " \n",
    "#packaging'\n",
    "def packaging(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if x.__contains__('no') and x.__contains__('nitrogen'):\n",
    "        return 'no nitrogen'\n",
    "    elif x.__contains__('not') and x.__contains__('n2'):\n",
    "        return 'no nitrogen'\n",
    "    elif x.__contains__('nitrogen') and not x.__contains__('no'):\n",
    "        return 'nitrogen'\n",
    "    elif x.__contains__('atmosphere'):\n",
    "        return 'atmosphere'\n",
    "    elif x==str(np.nan):\n",
    "        return 'NA'\n",
    "    elif bool(re.search('none|n/a|na',x)):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return(x)\n",
    "    \n",
    "train_df['packaging_label']=train_df['norm_packaging'].apply(lambda x: packaging(x))  \n",
    "  \n",
    "#'preservatives'\n",
    "def preservatives(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if x.__contains__('sodium'):\n",
    "        return 'Not used as preservatives'\n",
    "    elif x.__contains__('not') and x.__contains__('seasoning'):\n",
    "        return 'No Seasoning'\n",
    "    elif bool(re.search('topping|seasoning',x)):\n",
    "        return 'Used in seasoning'\n",
    "    else:\n",
    "        return ('NA')\n",
    "    \n",
    "train_df['preservatives_label']=train_df['norm_preservatives'].apply(lambda x:preservatives(x))\n",
    "\n",
    "#' otherFSA'\n",
    "def otherFSA(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if bool(re.search('moisture',x)):\n",
    "        return 'Moisture'\n",
    "    else:\n",
    "        return 'NA'\n",
    "train_df['otherFSA_label']=train_df['norm_otherFSA'].apply(lambda x:otherFSA(x))\n",
    "\n",
    "\n",
    "def foodsafety_prodclaims(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if bool(re.search('^claim',x)):\n",
    "        return 'Claims Made'\n",
    "    elif x.__contains__('no claims'):\n",
    "        return 'No Claims Made'\n",
    "    elif x.__contains__('allergen'):\n",
    "        return 'Allergen'\n",
    "    elif bool(re.search('none|n/a|na',x)):\n",
    "        return 'NA'\n",
    "    else:\n",
    "        return(x)\n",
    "\n",
    "train_df['foodSafety_prodClaims_label']=train_df['norm_foodSafety_prodClaims'].apply(lambda x:foodsafety_prodclaims(x))  \n",
    "\n",
    "#' targetMarket'\n",
    "def targetMarket(x):\n",
    "    x=str(x)\n",
    "    x=x.lower()\n",
    "    if (x.__contains__('choking') or x.__contains__('choke')) and x.__contains__('children'):\n",
    "        return 'Choking hazard for children'\n",
    "    elif x.__contains__('allerg'):\n",
    "        return 'Allergy'\n",
    "    elif x.__contains__('no') and x.__contains__('change'):\n",
    "        return 'no'\n",
    "    else:\n",
    "        return(x)\n",
    "train_df['targetMarket_label']=train_df['norm_targetMarket'].apply(lambda x:targetMarket(x))\n",
    "\n",
    "train_df=train_df.drop(['norm_preservatives', 'norm_pH', 'norm_waterActivity', 'norm_packaging','norm_otherFSA',\n",
    "            'norm_prodStorageDist', 'norm_foodSafety_prodClaims','norm_targetMarket'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onehot encoding of categorical encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,train_df.shape[1]):\n",
    "    if train_df.columns[i] != 'norm_allergens' :\n",
    "        if train_df.columns[i] != 'norm_allergens_M':\n",
    "        \n",
    "            df=pd.get_dummies(train_df[train_df.columns[i]])\n",
    "            df.columns = [train_df.columns[i]+\"_\"+k for k in df.columns]\n",
    "            if i ==0 :\n",
    "                newdf = df\n",
    "            else:\n",
    "                newdf = pd.concat([newdf,df],axis=1)\n",
    "                \n",
    "                \n",
    "#newdf.index = train_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(newdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf['norm_allergens']= train_df.norm_allergens\n",
    "newdf['norm_allergens_M']=train_df.norm_allergens_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_df.index = newdf.index\n",
    "train_df=newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imput target with mode value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics \n",
    "\n",
    "def impute_target(fsha_data,targetName):\n",
    "    train_y=[]\n",
    "    for i in range (len(fsha_data)):\n",
    "        if str(fsha_data[targetName][i]).strip().lower() =='yes':\n",
    "            train_y.append(1)\n",
    "        elif str(fsha_data[targetName][i]).strip().lower() =='no':\n",
    "            train_y.append(0)\n",
    "        else:\n",
    "            train_y.append(-1)\n",
    "               \n",
    "    mode_y = statistics.mode(train_y)\n",
    "\n",
    "    for i in range (len(fsha_data)):\n",
    "        if train_y[i]==-1:\n",
    "            train_y[i] = mode_y\n",
    "            \n",
    "    return train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target (Yes/No choice) in PDAF are converted to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_crosscontact = impute_target(fsha_data,\"crossContactAllergens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['target']=train_y_crosscontact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y = train_df['target']\n",
    "y = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform n-gram vectorization and PCA on text data and concatenate with categorical one-hot encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(train_df,y):    \n",
    "    train_labels = y\n",
    "    x_ngram_allergens = ngram_vectorize(train_df['norm_allergens'], train_labels,n_gram_range).toarray()\n",
    "    x_ngram_allergens_m = ngram_vectorize(train_df['norm_allergens_M'], train_labels,n_gram_range).toarray()\n",
    "    train_df = train_df.drop(['norm_allergens','norm_allergens_M'],axis=1)\n",
    "    pca = PCA(n_components=n_components,svd_solver=svd_solver,whiten=whiten, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x_ngram_allergens = scaler.fit_transform(x_ngram_allergens)\n",
    "    x_pca_allergens = pca.fit_transform(x_ngram_allergens)\n",
    "    x_pca_allergens_m = pca.fit_transform(x_ngram_allergens_m)\n",
    "    print(x_pca_allergens.shape)\n",
    "    print(x_pca_allergens_m.shape)\n",
    "    print(train_df.shape)\n",
    "    x_train = np.concatenate((train_df,x_pca_allergens,x_pca_allergens_m),axis=1)\n",
    "    print(x_train.shape)\n",
    "    return x_train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 2)\n",
      "(167, 2)\n",
      "(167, 55)\n",
      "(167, 59)\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (1,2)\n",
    "n_components = 2\n",
    "whiten = False\n",
    "random_state = 42\n",
    "svd_solver=\"full\"\n",
    "x_train = preprocess_text(train_df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = x_train,train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42,stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.columns = ['norm_newIngredient_no', 'norm_newIngredient_yes', 'pH_label_0',\n",
    "\n",
    "       'pH_label_3', 'prod_storageDist_label_Keep away from sun',\n",
    "\n",
    "       'prod_storageDist_label_NA', 'prod_storageDist_label_ambient',\n",
    "\n",
    "       'waterActivity_label_NA', 'waterActivity_label_as per current pc',\n",
    "\n",
    "       'waterActivity_label_low', 'waterActivity_label_max',\n",
    "\n",
    "       'waterActivity_label_not determined',\n",
    "\n",
    "       'waterActivity_label_not specified pd', 'packaging_label_NA',\n",
    "\n",
    "       'packaging_label_atmosphere', 'packaging_label_high barrier film',\n",
    "\n",
    "       'packaging_label_nitrogen', 'packaging_label_no different current tc',\n",
    "\n",
    "       'packaging_label_no nitrogen', 'packaging_label_not applicable',\n",
    "\n",
    "       'packaging_label_not confirmed pd', 'packaging_label_not provided',\n",
    "\n",
    "       'packaging_label_not required', 'packaging_label_standard',\n",
    "\n",
    "       'packaging_label_unknown', 'packaging_label_window clear film',\n",
    "\n",
    "       'packaging_label_wip bags part mix', 'preservatives_label_NA',\n",
    "\n",
    "       'preservatives_label_No Seasoning',\n",
    "\n",
    "       'preservatives_label_Not used as preservatives',\n",
    "\n",
    "       'otherFSA_label_Moisture', 'otherFSA_label_NA',\n",
    "\n",
    "       'foodSafety_prodClaims_label_50 less fat comparing fried potato chips',\n",
    "\n",
    "       'foodSafety_prodClaims_label_Claims Made',\n",
    "\n",
    "       'foodSafety_prodClaims_label_NA',\n",
    "\n",
    "       'foodSafety_prodClaims_label_No Claims Made',\n",
    "\n",
    "       'foodSafety_prodClaims_label_gluten free',\n",
    "\n",
    "       'foodSafety_prodClaims_label_no',\n",
    "\n",
    "       'foodSafety_prodClaims_label_no different current tc',\n",
    "\n",
    "       'foodSafety_prodClaims_label_no food safety related claims',\n",
    "\n",
    "       'foodSafety_prodClaims_label_no food safety related product claims',\n",
    "\n",
    "       'foodSafety_prodClaims_label_no preservatives',\n",
    "\n",
    "       'foodSafety_prodClaims_label_not applicable',\n",
    "\n",
    "       'foodSafety_prodClaims_label_product may contain peanut',\n",
    "\n",
    "       'foodSafety_prodClaims_label_tbc', 'targetMarket_label_Allergy',\n",
    "\n",
    "       'targetMarket_label_Choking hazard for children',\n",
    "\n",
    "       'targetMarket_label_kids 410', 'targetMarket_label_na',\n",
    "\n",
    "       'targetMarket_label_no', 'targetMarket_label_no vulnerable groups',\n",
    "\n",
    "       'targetMarket_label_none', 'targetMarket_label_not',\n",
    "\n",
    "       'targetMarket_label_not suitable people vulnerable gluten milk deretives','PCA_allergens_1','PCA_allergens_2','PCA_allergens_M_1','PCA_allergens_M_2','Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN ROC:0.584, precision @ rank n:0.9837\n",
      "[[ 1  0]\n",
      " [31  2]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.03      1.00      0.06         1\n",
      "          1       1.00      0.06      0.11        33\n",
      "\n",
      "avg / total       0.97      0.09      0.11        34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "# Train kNN detector\n",
    "clf = KNN(contamination=0.015, n_neighbors=5)\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Get the prediction labels of the training data\n",
    "y_train_pred = clf.labels_ \n",
    "    \n",
    "# Outlier scores\n",
    "y_train_scores = clf.decision_scores_\n",
    "\n",
    "# Import the utility function for model evaluation\n",
    "from pyod.utils import evaluate_print\n",
    "\n",
    "# Evaluate on the training data\n",
    "evaluate_print('KNN', y_train, y_train_scores)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hence we can conclude that using KNN anomaly dectector we can detect the deficient class in Cross Contact Allergens target column. The recall is 100%.With addition of more records, we can compute the proportion of \"No\" i.e.deficient class in Cross Contact Allergens column and detect all such records.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
